{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab09.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "title",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab 09: Random Variables and Inference & SQL\n",
    "For the first half of this lab, you will:\n",
    "\n",
    "1. Explore properties of random variables using the example of a binomial distribution.\n",
    "1. Verify the Central Limit Theorem (CLT) using simulations. \n",
    "1. Examine if the \"sample maximum\" is a biased estimator for the true maximum of a population.\n",
    "1. Perform inference for the population correlation of the tips dataset.\n",
    "\n",
    "For the second half of the lab, you will practice viewing, sorting, grouping, and merging tables with SQL. We will explore two datasets:\n",
    "1. A \"minified\" version of the [Internet Movie Database](https://www.imdb.com/interfaces/) (IMDb). This SQLite database (~10MB) is a tiny sample of the much larger database (more than a few GBs). As a result, disclaimer that we may get wildly different results than if we use the whole database!\n",
    "\n",
    "1. The money donated during the 2016 election using the [Federal Election Commission (FEC)'s public records](https://www.fec.gov/data/). You will be connecting to a SQLite database containing the data. The data we will be working with in this lab is relatively small (~106MB); however, it is a sample taken from a much larger database (more than a few GBs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To receive credit for a lab, answer all questions correctly and submit before the deadline.\n",
    "\n",
    "You must submit this assignment to Pensieve by the on-time deadline, **Thursday, July 31, 11:59 PM PT**. Please read the syllabus for the Slip Day policy. As a reminder, slip days are **not** applicable on labs. **We strongly encourage you to plan to submit your work to Pensieve several hours before the stated deadline.** This way, you will have ample time to contact staff for submission support. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Walk-Through\n",
    "In addition to the lab notebook, we have also released a prerecorded walk-through video of the lab. We encourage you to reference this video as you work through the lab. Run the cell below to display the video.\n",
    "\n",
    "**Note**: The walkthrough video is partially recorded from Spring 2023. There may be slight inconsistencies between the version you are viewing and the version used in the recording, but content is identical.\n",
    "\n",
    "**Playlist link**: click [**here**](https://www.youtube.com/watch?v=_K7OvmRbb5w&list=PLQCcNQgUcDfoJvVQxlWZC2vJfj1IKzWwR) for all the parts of the lab.\n",
    "\n",
    "For the first half:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"_K7OvmRbb5w\", list = 'PLQCcNQgUcDfoJvVQxlWZC2vJfj1IKzWwR', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second half:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo(\"uQ3E4pejmD8\", list = 'PLQCcNQgUcDfpdBnhS-lPq8LPas48tkMgp', listType = 'playlist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 10 & 11 Mini-lecture\n",
    "\n",
    "In Data 100, discussions will not reserve time to host mini-lectures. Instead, we will release a set of pre-recorded mini-lectures that supplement the concepts introduced in lecture. For labs released on Tuesday (or earlier), the content of the mini-lectures will be covered in Wednesday discussions. For lab released on Thursday (or earlier), the content of the mini-lecture will be covered in Monday discussions. Thus, it is important to watch these mini-lectures **before** attending the discussion section you are assigned to. Discussion 10 mini-lecture is attached [here](https://www.youtube.com/watch?v=bp7-OwxdGwg&list=PLQCcNQgUcDfoUXRtrHc9TUx2pBYNfToVN). Discussion 11 mini-lecture is attached [here](https://www.youtube.com/watch?v=bp7-OwxdGwg&list=PLQCcNQgUcDfoUXRtrHc9TUx2pBYNfToVN)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about this assignment, we ask that you **write your solutions individually**. If you discuss the assignment with others, please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *list names here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Debugging Guide\n",
    "\n",
    "If you run into any technical issues, we highly recommend checking out the [Data 100 Debugging Guide](https://ds100.org/debugging-guide/). In this guide, you can find general questions about Jupyter notebooks / Datahub, Pensieve, and common `pandas`, RegEx, and visualization errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "setup",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('path_to_file/filename.extension')\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "sns.set_context(\"talk\")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(2023) # Do not change this line; this sets the pseudorandomness of the autograder.\n",
    "\n",
    "from IPython.display import display, Latex, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 1: Random Variables and Inferences\n",
    "---\n",
    "\n",
    "## Question 1: Probability with Binomial Random Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "### Question 1a: Loading the Data\n",
    "\n",
    "The Berkeley Half Marathon is an annual weekend-long race here in Berkeley. \n",
    "\n",
    "We want to understand how many participants in this year’s race also participated in the previous year's race. To accomplish this, we collect a sample of this year’s participants.\n",
    "\n",
    "Let's first assume that we have access to the official data so we can simulate the potential result we might get (**in practice we don't!**). The dataset `marathon.csv` includes information for **all racers** who registered for the Berkeley Half Marathon. In other words, the dataset represents our **full population**.\n",
    "\n",
    "* The `Bib Number` of each participant (i.e., racer) is in order of registration — integers from $1$ to the total unknown number of participants. \n",
    "* The column `Race Type` denotes the type of race a participant is in.\n",
    "* The column `Experienced` denotes if a participant participated in the race in the previous year.\n",
    "* The column `Dog Lover` denotes if a participant is a dog lover.\n",
    "\n",
    "Load the dataset `marathon.csv` into the `DataFrame` `marathon` and assign `true_prop` to the true proportion of experienced racers. \n",
    "\n",
    "**Hint:** The **true proportion** of experienced racers is the proportion of experienced racers in the *population*. We term numerical functions of the population, such as the true proportion of experienced racers, as **population parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "marathon = ...\n",
    "display(marathon.head())\n",
    "true_prop = ...\n",
    "print(f\"The true proportion of experienced racers in the population of size {len(marathon)} is {np.round(true_prop, 4)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Suppose that you have access to the official roster and are able to collect a Simple Random Sample (SRS) of 100 racers. You decided to use the proportion of experienced racers in this smaller *sample* as an **estimate** of the true proportion of experienced racers in the full *population*. Let's denote this true proportion as $p$.\n",
    "\n",
    "\n",
    "How would a sample proportion compare to the true proportion? Suppose we take a simple random sample of size $n$. For an individual $i \\in \\{1, 2, \\dots, n\\}$ in our sample, we define $X_i$ to be a random variable indicating if individual $i$ is experienced or not. That is, if individual $i$ is experienced, $X_i = 1$, otherwise, $X_i = 0$. Then we can define the sample proportion as the fraction of experienced racers in the sample. The sample proportion $\\hat{p}$ is therefore also the mean of the sample.\n",
    "\n",
    "$$\\hat{p} = \\text{sample proportion} = \\frac{1}{n}\\sum_{i=1}^{\\text{n}} X_i$$\n",
    "\n",
    "Note that **sample proportion** is a numerical function of the sample, so it is also a (sample) statistic. As a reminder, sample statistics are random variables, due to the randomness of the samples. \n",
    "\n",
    "\n",
    "For the remainder of the lab, we will assume that the true population is large enough to simplify the sample as **a random sample with replacement.** Under this assumption, $X_i$’s are i.i.d (independent and identically distributed). Each $X_i$ follows a Bernoulli distribution with probability $p$ that a racer is experienced (i.e. $P(X_i = 1) = p$). Then, the sample proportion $\\hat{p}$ is a scaled Binomial random variable with expectation $\\mathbb{E}[\\hat{p}] = p$ and variance $\\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n}$.\n",
    "\n",
    "As a reminder, we can show that $\\mathbb{E}[\\hat{p}] = p$ via linearity of expectation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[\\hat{p}] = \\mathbb{E}[\\frac{1}{n}\\sum_{i=1}^{\\text{n}} X_i] = \\frac{1}{n}\\sum_{i=1}^{\\text{n}} \\mathbb{E}[X_i] = \\frac{1}{n}\\sum_{i=1}^{\\text{n}} p = \\frac{np}{n} = p\n",
    "$$\n",
    "\n",
    "and $\\text{Var}(\\hat{p}) = \\frac{p(1-p)}{n}$ via additivity of variance in independent variables:\n",
    "\n",
    "$$\n",
    "\\text{Var}(\\hat{p}) = \\text{Var}(\\frac{1}{n}\\sum_{i=1}^{\\text{n}} X_i) = \\frac{1}{n^2}\\sum_{i=1}^{\\text{n}} \\text{Var}(X_i) = \\frac{1}{n^2}\\sum_{i=1}^{\\text{n}} p(1-p) = \\frac{p(1-p)}{n}\n",
    "$$\n",
    "\n",
    "In the remainder of this question, let's confirm these statistics through simulation.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1b: Expected Proportion\n",
    "\n",
    "The expressions above give us the expectation and variance for the proportion of experienced racers if we apply probability theory. Do these results hold true if we actually simulate the proportion of experienced racers on random samples?”\n",
    "\n",
    "Run 5000 independent simulations to compute the **proportion of experienced racers** in simulated samples of size $n = 100$, each generated uniformly at random from the true population `marathon`. You may assume that the true population is large enough such that the sample is a random sample with replacement. Assign `samples` to an array with 5000 elements, each of which is the proportion of experienced racers in that simulated sample. Also, assign `simulated_mean` and `simulated_var` to the mean and variance of the simulated proportions, respectively.\n",
    "\n",
    "Useful function: `df.sample` ([link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "samples = ...\n",
    "simulated_mean = ...\n",
    "simulated_var = ...\n",
    "\n",
    "print(f\"[Mean]     Simulated: {simulated_mean:.5f}   Theoretical: {true_prop:.5f}\")\n",
    "print(f\"[Variance] Simulated: {simulated_var:.5f}   Theoretical: {true_prop*(1-true_prop)/100:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "What is a better way to support racers than passing out dog photos? :-) You decide to take a sample of size $n = 100$, where each racer will receive 1 dog photo if they are an experienced racer, 3 dog photos if they love dogs, and 4 dog photos if they are both an experienced racer and love dogs. What is the expected number of photos you need to print? \n",
    "\n",
    "Again, assume that the true population is large enough such that the sample is a random sample with replacement to simplify the problem, and that whether a racer loves dogs and whether the racer participated in the previous year are independent. Let $D$ be the number of dog photos that need to be printed. Here, we picked $D$ to refer to dog photos. More generally, when picking letters for random variables, it is good practice to try and pick something informative and unambiguous. \n",
    "\n",
    "We can then find the **expected number of photos, $\\mathbb{E}(D)$** as follows: \n",
    "$$\\mathbb{E}(D) = \\large 100 \\cdot p + 100 \\cdot 3 \\cdot q,$$ \n",
    "where $p$ is the true proportion of experienced racers and $q$ is the true proportion of dog lovers. This result follows from the linearity of expectation:\n",
    "\n",
    "$$\\mathbb{E}[aX+bY] = a \\mathbb{E}[X] + b\\mathbb{E}[Y].$$\n",
    "\n",
    "The variance of the number of photos is:\n",
    "$$\\text{Var} (D) = \\large 100 \\cdot p (1- p)+ 100 \\cdot 3^2 \\cdot q (1- q),$$\n",
    "which follows from the properties of variance and that the two samples are independent: \n",
    "\n",
    "$$\\text{Var}(aX+bY) = a^2\\text{Var}(X) + b^2\\text{Var}(Y) + 2 \\cdot a \\cdot b \\text{Cov}(X, Y) = a^2\\text{Var}(X) + b^2\\text{Var}(Y).$$\n",
    "\n",
    "See the video walkthrough for a full derivation of these results.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 1c: Expectation and Variance of Linear Combinations of Random Variables\n",
    "\n",
    "Confirm this result through simulation. Run 5000 independent simulations, where each simulation finds the number of **photos** needed for a sample of size $n = 100$ racers, selected uniformly at random (with replacement) from the true population `marathon`.  Assign `photo_samples` to an array with 5000 elements, each of which is the number of **photos** needed for the simulated sample. \n",
    "\n",
    "**Note:** We have computed `prop_dog_lover`, the true proportion of dog lover racers for you so that you can verify that your simulated statistics match the theoretical statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "prop_dog_lover = np.mean(marathon[\"Dog Lover\"])\n",
    "\n",
    "photo_samples = []\n",
    "...\n",
    "\n",
    "simulated_photos_mean = np.mean(photo_samples)\n",
    "simulated_photos_var = np.var(photo_samples)\n",
    "\n",
    "\n",
    "print(f\"[Mean]     Simulated: {simulated_photos_mean:.5f}    Theoretical: {(true_prop + 3*prop_dog_lover)*100:.5f}\")\n",
    "print(f\"[Variance] Simulated: {simulated_photos_var:.5f}   Theoretical: {100*true_prop*(1-true_prop) + 900*prop_dog_lover*(1-prop_dog_lover):.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 2: Central Limit Theorem\n",
    "\n",
    "The Central Limit Theorem states that the distribution of the sample mean will converge to a normal distribution as the sample size ($n$) goes to infinity. That means that if we collected enough samples from the population, calculated the proportion of experienced racers (which is a sample mean) for each sample, and viewed a histogram of the proportions, we would see a normal distribution!\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2a\n",
    "Complete the function `simulate`. The function `simulate` takes in one argument (`sample_size`: the size of the sample) and returns a list of length 5000 where each element is the proportion of experienced racers in a random sample of size `sample_size`. \n",
    "\n",
    "Then, use `simulate` to run 5000 independent simulations, where each simulation finds the proportion of **experienced racers** in a sample of size of 100, 500, and 1000 selected uniformly at random from the true population `marathon`. You may assume that the true population is large enough such that the sample is a random sample with replacement (note that in reality, our population is finite in size about 50k; this approximation becomes more inaccurate as our sample size grows larger). You should assign `samples100`, `samples500`, and `samples1000` each to arrays of 5000 elements with proportions of experienced racers of sample sizes **100**, **500**, and **1000**, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def simulate(sample_size):\n",
    "    ...\n",
    "\n",
    "samples100 = ...\n",
    "samples500 = ...\n",
    "samples1000 = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2b\n",
    "\n",
    "Recall that if a random variable follows a normal distribution with mean $\\mu$ and variance $\\sigma^2$, then its Probability Density Function (pdf) is\n",
    "$$\\large\n",
    "f(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2  \\sigma ^2} \\right)\n",
    "$$\n",
    "\n",
    "Complete the function `gaussian` which returns the pdf of a normal distribution with mean `mean`, and variance of `var`, computed at values `x`. Pay attention to the order of operations and add parentheses accordingly - the solution presented in the lab walkthrough video is missing a pair, so you will need to modify the solution slightly!\n",
    "\n",
    "**Hint:** This is very similar to `gaussian_kernel` from Lab 04!\n",
    "\n",
    "**Note:** The solution differs slightly from the walkthrough. You will need to add parentheses around `2 * var` for the tests to pass.\n",
    "\n",
    "**Another Note:** Use the `numpy` (instead of `math`) library to access common mathematical constants, such as $e$ and $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def gaussian(mu, var, x):\n",
    "    \"\"\"\n",
    "    Compute the Gaussian density at value x.\n",
    "\n",
    "    Args:\n",
    "        mu: the mean/center of the Gaussian distribution.\n",
    "        var: variance of the Gaussian distribution.\n",
    "        x: observation.\n",
    "\n",
    "    Returns:\n",
    "        The density at value x.\n",
    "    \"\"\"    \n",
    "    ...\n",
    "\n",
    "gaussian(0, 1, np.array([-1, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 2c\n",
    "\n",
    "We are ready to demonstrate the Central Limit Theorem visually by comparing simulated distributions of sample means to the normal distribution. We have provided the skeleton code of an interactive plot. Fill in the blanks below using the `simulate` and `gaussian` functions from previous parts of this question.\n",
    "\n",
    "Then, in the cell below, describe the mean and spread of the sampling distribution and how they change as you increase the value of `sample_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "def f(sample_size):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    # Generate the simulated proportions\n",
    "    sim_samples = ...\n",
    "    # Make a histogram plot of the simulated proportions. Set density to True and edgecolor to \"none\"\n",
    "    ...\n",
    "    x = np.linspace(0, 1, 1001)\n",
    "    # We provided the mean and variance for you. If you are interested in knowing how to calculate these, take Data 140!\n",
    "    mean = true_prop\n",
    "    var = true_prop*(1-true_prop)/sample_size\n",
    "    # Compute the pdf of the normal distribution of mean `mean` and variance `var` at locations x\n",
    "    y = ...\n",
    "    plt.plot(x, y, linewidth=1)\n",
    "    plt.xlim(0, 0.6);\n",
    "    plt.ylim(0, 35);\n",
    "    plt.show()\n",
    "interact(f, sample_size=(10, 1000, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 3: Estimator for Population Max\n",
    "\n",
    "\n",
    "Now suppose that we do not have access to the official roster; instead, we only have one sample. Without the official roster, we do not know the population and therefore do not know the total number of racers. However, we still want to estimate the total racers given an observed sample so we can prepare a dog photo for everyone. That is, we want to find an estimator for the **population maximum**.\n",
    "\n",
    "Recall that the `Bib Number` of each participant (i.e., racer) is in order of registration—integers from $1$ to the total unknown number of participants. You decide to construct a sample by recording the bib number of every racer you see on the street in a given time period and use the maximum bib number in your sample as an estimator for the true maximum bib number (that is, the total number of participants, assuming everyone who registered participated). Assume that a racer's bib number has no relation to their racing experience so that you are equally likely to see any of the bib numbers in your sample.\n",
    "\n",
    "**Is the sample maximum a good estimator for the population maximum?** We'll use simulation to explore the answer to this question in this part of the lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3a\n",
    "\n",
    "Let's first assume that we have access to the total number of participants (again, in practice we don't!). Find the **true population maximum** and assign it to `true_max`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "true_max = ...\n",
    "true_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the summary statistics of Bib Number; no further action is needed.\n",
    "marathon.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the above output to quickly check and see if the value you assigned to `true_max` aligns with what you find in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3b\n",
    "\n",
    "How would a sample maximum compare to the true maximum? Suppose we draw a sample of size $n$ with replacement from the population. We denote the bib number of individual $i$ in the sample as $B_i$. We will have $n$ i.i.d. random variables: $B_1, B_2, \\dots, B_n$. Define the **sample max** as the maximum value of the sample.\n",
    "\n",
    "$$\\text{sample max} = \\max (B_1, \\dots, B_n)$$\n",
    "\n",
    "\n",
    "Recall from [Data 8](https://inferentialthinking.com/chapters/10/3/Empirical_Distribution_of_a_Statistic.html) that we can get the empirical distribution of a statistic by **simulating**, or repeatedly sampling from the population.\n",
    "Suppose we compute the sample max as the **maximum bib number from observing the bib numbers of $n = 200$ random racers**. By repeating this process for many randomly selected samples, we get a simulated distribution of the sample max statistic.\n",
    "\n",
    "Assign `sample_maxes` to a list that contains 5000 simulated sample maxes from samples of size $n = 200$, each sampled randomly **with replacement** from the population `marathon`. (Side note: We sample with replacement because while it suggests that we could see the same racer multiple times in our sample, it allows us to assume each individual in our sample is drawn i.i.d. from the population.)\n",
    "\n",
    "Some useful functions: `df.sample` ([link](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html)), `np.random.choice` ([link](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html)). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "q2_code",
     "locked": false,
     "schema_version": 2,
     "solution": true
    },
    "tags": [
     "student",
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "sample_maxes = ...\n",
    "for i in range(5000):\n",
    "    sample = ...\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Question 3c\n",
    "\n",
    "Plot the empirical distribution of the sample maximum that you generated in Question 3b. Your plot should look like the below plot. It should include both the average sample maximum and the true population maximum as vertical lines.\n",
    "\n",
    "<img src='images/sample_max_dist.png' width=\"600px\" />\n",
    "\n",
    "Visualization/plotting tips:\n",
    "* To plot a vertical line with specific linestyles, see the `plt.axvline` [documentation](https://matplotlib.org/3.5.1/api/_as_gen/matplotlib.pyplot.axvline.html).\n",
    "* To include a label in the legend, pass in `label=...` to the plot that you'd like to label ([example](https://matplotlib.org/3.5.1/gallery/pyplots/axline.html#sphx-glr-gallery-pyplots-axline-py)).\n",
    "\n",
    "**Note:** Your graph does not need to look exactly like the one above, but it should be similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = [10, 6])\n",
    "bins = np.linspace(49000, 50750, 25) # For your plot\n",
    "\n",
    "avg_sample_maxes = ...\n",
    "...\n",
    "\n",
    "plt.legend();     # Show legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "</br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 3d\n",
    "\n",
    "Recall from Lecture 16 that an **unbiased estimator** is one where the expected value of the estimator is the parameter. For example, the sample mean $\\bar{X}_n$ is an unbiased estimator of the population mean $\\mu$ because $\\mathbb{E}[\\bar{X}_n] = \\mu$ by linearity of expectation.\n",
    "\n",
    "Based on your analysis in Question 3c, assign `q3d` to the most correct option out of the following; then in the second cell, **explain your choice.**\n",
    "\n",
    "1. The sample maximum is an unbiased estimator of the population maximum.\n",
    "1. The sample maximum overestimates the population maximum.\n",
    "1. The sample maximum underestimates the population maximum.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "q3d = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation your choice here**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Question 4: Inference for the Population Correlation\n",
    "\n",
    "\n",
    "Previously in this lab, we explored some properties of random variables and identities a biased estimator. For these parts, we assumed we had access to the population data. We simulate samples from the **true population** and calculate the mean and variance of these sample statistics. In practice, however, we only have access to one sample (and therefore one value of our estimator); we will explore this next.\n",
    "\n",
    "\n",
    "We define **population correlation** as the expected product of *standardized* deviations from expectation: \n",
    "\n",
    "$$r(X, Y) =  \\mathbb{E} \\left[\\left(\\frac{X - \\mathbb{E}[X]}{\\text{SD}(X)} \\right) \\left(\\frac{Y - \\mathbb{E}[Y]}{\\text{SD}(Y)}\\right)\\right]$$\n",
    "\n",
    "Note that population correlation involves the population means $\\mathbb{E}[X]$ and $\\mathbb{E}[Y]$ and the population standard deviations $\\text{SD}(X)$ and $\\text{SD}(Y)$. Correlation provides us with important information about the linear relationship between variables.\n",
    "\n",
    "In this part, we'll explore the `tips` dataset once more, and we will compute the sample correlation statistic of two features: **total bill** and **party size**. We will then explore how the sample correlation estimates the true population correlation parameter.\n",
    "\n",
    "The below cell assigns `data` to our single sample collected about customer tipping behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to load tips data\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "data = tips[['total_bill','size']]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4a\n",
    "To estimate the population correlation, we'd like to use an estimator based on data from a simple random sample of our tips data set. For a sample $(X_1, Y_1), \\dots, (X_n, Y_n)$ generated IID from a population,  define the **sample correlation** as follows:\n",
    "\n",
    "$$\\frac{\\sum\\limits_{i=1}^n\\left(X_i-\\overline{X}\\right)\\left(Y_i-\\overline{Y}\\right)}{\\sqrt{\\sum\\limits_{i=1}^n \\left(X_i - \\overline{X}\\right)^2}\\sqrt{\\sum\\limits_{i=1}^n \\left(Y_i - \\overline{Y}\\right)^2}}$$\n",
    "\n",
    "In this formula, the numerator calculates the covariance between the two variables, while the denominator normalizes this value, yielding a coefficient between -1 and 1 that represents the strength and direction of the linear relationship in the sample data.\n",
    "\n",
    "Note the similar structure to the true population correlation. If the $i$-th individual in our sample has \"total bill\" $X_i$ and \"party size\" $Y_i$, then $\\overline{X}, \\overline{Y}$ are the sample means of total bill and party size, respectively.\n",
    "\n",
    "Implement the `sample_correlation` function in the cell below to compute the sample correlation for `sample`, which has two columns: `total_bill` and `size`.\n",
    "\n",
    "**Hint:** Remember that Python evaluates mathematical expressions the same way a mathematician would. For example, $A / B * C$ is interpreted as $(A / B) * C$, not $A / (B * C)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def sample_correlation(sample):\n",
    "    \"\"\"\n",
    "    Compute sample correlation of x and y.\n",
    "    sample: A DataFrame of dimension (n, 2). The two columns are 'total_bill' and 'size'\n",
    "    \"\"\"\n",
    "    x, y = sample['total_bill'], sample['size']\n",
    "    x_bar = ...\n",
    "    y_bar = ...\n",
    "    n = ...\n",
    "    ...\n",
    "\n",
    "sample_correlation(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review: Terminology\n",
    "\n",
    "Let the sample correlation of `data` be the estimator for the population correlation. In other words:\n",
    "\n",
    "* **Parameter**: Population correlation. Unknown, but fixed.\n",
    "* **Statistic**: Sample correlation. Dependent on the random sample we obtained.\n",
    "* **Estimator**: The sample correlation statistic `corr_est` is an estimator of the population correlation parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we infer about the population correlation given this estimate? Is it possible that the total bill and the party size are actually uncorrelated?\n",
    "\n",
    "We can perform **bootstrapped hypothesis testing** ([data 8 textbook](https://inferentialthinking.com/chapters/13/2/Bootstrap.html)): We cannot simulate samples from the true population, what if we resample from the sample instead? If the sample is representative of the true population, then the resample will also be similar to samples from the original population. Note that if your sample is not representative, bootstrapping will also give you a biased result.\n",
    "\n",
    "The hypotheses are as follows:\n",
    "\n",
    "* **Null hypothesis**: Total bill and party size are uncorrelated; the population correlation is 0.\n",
    "* **Alternate hypothesis**: The population correlation is not 0.\n",
    "\n",
    "\n",
    "To test this hypothesis, we can bootstrap a $(1-p)$% confidence interval for the population correlation and check if 0 is in the interval. If 0 is in the interval, the data are consistent with the null hypothesis. If 0 is *not* in the interval, we reject the null hypothesis at the $p$% significance level. For more on the duality of the confidence interval and the p-value, see this [StackExchange discussion](https://stats.stackexchange.com/questions/179902/confidence-interval-p-value-duality-vs-frequentist-interpretation-of-cis). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4b\n",
    "\n",
    "Implement the `ci_correlation` function in the cell below that returns a bootstrapped confidence interval at the `conf` level. Your bootstrap should resample the `sample` `DataFrame` with replacement `n` times to construct `m` bootstrapped sample correlations using the `sample_correlation` function you implemented in Question 4a.\n",
    "\n",
    "Then, assign `boot_ci` to the bootstrapped 95\\% confidence interval for the tips `data` sample.\n",
    "\n",
    "**Hint:** You may find `np.percentile`([documentation](https://numpy.org/doc/stable/reference/generated/numpy.percentile.html)) helpful.\n",
    "\n",
    "**Note:** This question is called Question 5 in the lab walkthrough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "def ci_correlation(sample, conf, m=5000):\n",
    "    \"\"\"Compute a bootstrap confidence interval for the correlation coefficient.\n",
    "    Parameters:\n",
    "    sample (DataFrame): A DataFrame containing at least two numeric columns to compute correlation.\n",
    "    conf (float): The desired confidence level (e.g., 95 for a 95% confidence interval).\n",
    "    m (int): The number of bootstrap resamples to generate. Default is 5000.\n",
    "    \"\"\"\n",
    "    estimates = []\n",
    "    n = len(sample)\n",
    "    for j in range(m):\n",
    "        resample = ...\n",
    "        ...\n",
    "    lower = ...\n",
    "    upper = ...\n",
    "    return (lower, upper)\n",
    "\n",
    "boot_ci = ...\n",
    "boot_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4c\n",
    "Now that we have the bootstrapped 95% confidence interval of the parameter based on a single sample of size 244, let's determine what we can conclude about our population correlation.\n",
    "\n",
    "Fill in the blanks for the sentence:\n",
    "\n",
    "> By bootstrapping our sample `data`, our estimate of the population correlation is ________ with a ___ % confidence interval of ________."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 4d\n",
    "\n",
    "In the cell below, interpret the statement from the previous question. Can we reject the null hypothesis at the 5% significance level? What can we infer about the relationship between total bill and party size?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "# Part 2: SQL\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Note!\n",
    "\n",
    "**Please only have ONE active SQL database running at once on DataHub, meaning do not have multiple homework/lab/lecture notebooks running at the same time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "if Path('../../../../../../../gradescope').is_dir():\n",
    "    path = Path('.') \n",
    "else:\n",
    "    path = Path('../../../../../../../tmp') \n",
    "\n",
    "with ZipFile('data.zip', 'r') as zipObj:\n",
    "    # Extract all the contents of zip file to the tmp directory\n",
    "    zipObj.extractall(path = path)\n",
    "\n",
    "imdb_sqlite_conn = 'duckdb:///' + str(path / 'imdbmini.db')\n",
    "fec_nyc_sqlite_conn = 'duckdb:///' + str(path / 'fec_nyc.db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL Query Syntax\n",
    "\n",
    "Throughout this lab, you will become familiar with the following syntax for the `SELECT` query:\n",
    "\n",
    "```\n",
    "SELECT <column list>\n",
    "FROM <table>\n",
    "[WHERE <predicate>]\n",
    "[GROUP BY <column list>]\n",
    "[HAVING <predicate>]\n",
    "[ORDER BY <column list>]\n",
    "[LIMIT <number of rows>]\n",
    "[OFFSET <number of rows>]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subpart 1 [Tutorial]: Writing SQL in Jupyter Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution: Be careful with large SQL queries!!** You may need to reboot your Jupyter Hub instance if it stops responding. To avoid printing out 100k-sized tables, we've adjusted the display limit to ensure that the tables displayed are truncated to 20 rows (though they may contain more rows in reality).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config SqlMagic.displaylimit = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up SQL. \n",
    "import duckdb\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. `%%sql` cell magic\n",
    "\n",
    "In lecture, we used the `sqlalchemy` extension to use **`%%sql` cell magic**, which enables us to connect to SQL databases and issue SQL commands within Jupyter Notebooks.\n",
    "\n",
    "Run the below cells to connect to a mini IMDb database using `duckdb` as the backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to connect to duckdb\n",
    "conn = duckdb.connect()\n",
    "conn.query(\"INSTALL sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to connect to the imdbmini database\n",
    "imdb_mini_db = 'duckdb:///' + str(path / 'imdbmini.db')\n",
    "%sql duckdb:///../../../../../../../tmp/imdbmini.db --alias imdb_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above cell connects to the same database using the SQLAlchemy Python library, which can connect to several different database management systems, including sqlite3, MySQL, PostgreSQL, and Oracle; we use `duckdb`. The library also supports an advanced feature for generating queries called an [object relational mapper](https://docs.sqlalchemy.org/en/20/tutorial/index.html#unified-tutorial) or ORM, which we won't discuss in this course but is quite useful for application development. \n",
    "                                                                                                                                                                                                                                                                                                                                   <br><br>\n",
    "                                                                                                                                                                                                                                                          Above, prefixing our single-line command with `%sql` means that the entire line will be treated as a SQL command (this is called \"line magic\"). In this class we will most often write multi-line SQL, meaning we need \"cell magic\", where the first line has `%%sql` (note the double `%` operator).\n",
    "\n",
    "The database `imdbmini.db` includes several tables, one of which is `Name`. Running the below cell will return first 5 lines of that table. Note that `%%sql` is on its own line.\n",
    "\n",
    "We've also included syntax for single-line comments, which are surrounded by `--`, and multi-line comments, which are surrounded by `/*` and `*/`.\n",
    "                                                                                                                                                                                                                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "/*\n",
    " * This is a\n",
    " * multi-line comment.\n",
    " */\n",
    "-- This is a single-line/inline comment. --\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  Name\n",
    "LIMIT\n",
    "  5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "### 2. The `pandas` command `pd.read_sql`\n",
    "\n",
    "This section describes how data scientists use SQL and `python` in practice, using the `pandas` command `pd.read_sql` ([documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_sql.html)). **You will see both `%sql` magic and `pd.read_sql` in this course**.\n",
    "\n",
    "With the SQLAlchemy object `engine`, we can call `pd.read_sql` which takes in a `query` **string**. Note the `\"\"\"` to define our multi-line string, which allows us to have a query span multiple lines. The resulting `DataFrame` `df` stores the results of the same SQL query from the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the demo.\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM name\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, imdb_mini_db)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `pd.read_sql` vs. `%sql` magic error messages\n",
    "\n",
    "`pd.read_sql` has **long error messages**: Given that the SQL query is now in the string, the errors become more unintelligible. Consider the below (incorrect) query.\n",
    "\n",
    "**Note**: Uncomment the below code and check out the error. You can uncomment/comment out multiple cells at the same time by selecting the lines and press ctrl+/ or command+/. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below code and check out the error.\n",
    "# query = \"\"\"\n",
    "# SELECT *\n",
    "# FROM Title;\n",
    "# LIMIT 5\n",
    "# \"\"\"\n",
    "# pd.read_sql(query, imdb_mini_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<details>\n",
    "<summary>Now that's an unruly error message! Can you see what's wrong in the cell above and correct the query? Toggle this cell to check your answer!</summary>\n",
    "It has a semicolon in the wrong place!\n",
    "</details>\n",
    "\n",
    "<br/>\n",
    "\n",
    "On the other hand, `%sql` magic gives more intelligible error messages, so we will use this format more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%sql\n",
    "# -- Uncomment all this block and check out the error. --\n",
    "# SELECT *\n",
    "# FROM Title;\n",
    "# LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Subpart 1: The IMDb (mini) Dataset\n",
    "\n",
    "Let's explore a miniature version of the [IMDb Dataset](https://www.imdb.com/interfaces/). This is the same dataset that we will use for the upcoming homework. We'll load it in using cell magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql imdb_engine\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  sqlite_master\n",
    "WHERE\n",
    "  type = 'table';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From running the above cell, we see the database has 4 tables: `Title`, `Name`, `Role`, and `Rating`.\n",
    "\n",
    "<details open>\n",
    "    <summary>[<b>Click to Expand</b>] See descriptions of each table's schema.</summary>\n",
    "    \n",
    "**`Title`** - Contains the following information for titles.\n",
    "    \n",
    "- tconst (integer) - alphanumeric unique identifier of the title\n",
    "- titleType (text) -  the type/format of the title\n",
    "- primaryTitle (text) -  the more popular title / the title used by the filmmakers on promotional materials at the point of release\n",
    "- originalTitle (text) -  original title, in the original language\n",
    "- isAdult (text) - 0: non-adult title; 1: adult title\n",
    "- startYear (text) – represents the release year of a title.\n",
    "- endYear (text) - TV Series end year. 'None' for all other title types\n",
    "- runtimeMinutes (text)  – primary runtime of the title, in minutes\n",
    "- genres (text) – includes up to three genres associated with the title\n",
    "\n",
    "\n",
    "**`Name`** – Contains the following information for names of people.\n",
    "    \n",
    "- nconst (integer) - alphanumeric unique identifier of the name/person\n",
    "- primaryName (text)– name by which the person is most often credited\n",
    "- birthYear (text) – in YYYY format\n",
    "- deathYear (text) – in YYYY format\n",
    "- primaryProfession (text) – the top-3 professions of the person\n",
    "    \n",
    "    \n",
    "**`Role`** – Contains the principal cast/crew for titles.\n",
    "    \n",
    "- tconst (integer) - alphanumeric unique identifier of the title\n",
    "- ordering (text) – a number to uniquely identify rows for a given tconst\n",
    "- nconst (integer) - alphanumeric unique identifier of the name/person\n",
    "- category (text) - the category of job that person was in\n",
    "- job (text) - the specific job title if applicable, else 'None'\n",
    "- characters (text) - the name of the character played if applicable, else 'None'\n",
    "    \n",
    "**`Rating`** – Contains the IMDb rating and votes information for titles.\n",
    "    \n",
    "- tconst (integer) - alphanumeric unique identifier of the title\n",
    "- averageRating (text) – weighted average of all the individual user ratings\n",
    "- numVotes (text) - number of votes (i.e., ratings) the title has received\n",
    "    \n",
    "</details>\n",
    "\n",
    "<br/><br/>\n",
    "\n",
    "From the above descriptions, we can conclude the following:\n",
    "* `Name.nconst` and `Title.tconst` are primary keys of the `Name` and `Title` tables, respectively.\n",
    "* that `Role.nconst` and `Role.tconst` are **foreign keys** that point to `Name.nconst` and `Title.tconst`, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 5\n",
    "\n",
    "What are the different kinds of `titleType`s included in the `Title` table? Write a query to find out all the unique `titleType`s of films using the `DISTINCT` keyword.  (**You may not use `GROUP BY`.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%sql imdb_engine --save query_q5\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for grading purposes. \n",
    "# No further action is required. \n",
    "sql_q5 = %sqlcmd snippets query_q5\n",
    "res_q5 = pd.read_sql(sql_q5, imdb_mini_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 6\n",
    "\n",
    "Before we proceed we want to get a better picture of the kinds of jobs that exist.  To do this examine the `Role` table by computing the number of records with each job `category`.  Present the results in descending order by the total counts.\n",
    "\n",
    "The top of your table should look like this (however, you should have more rows):\n",
    "\n",
    "| |category|total|\n",
    "|-----|-----|-----|\n",
    "|**0**|actor|21665|\n",
    "|**1**|writer|13830|\n",
    "|**2**|...|...|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%sql imdb_engine --save query_q6\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for grading purposes. \n",
    "# No further action is required. \n",
    "sql_q6 = %sqlcmd snippets query_q6\n",
    "res_q6 = pd.read_sql(sql_q6, imdb_mini_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "If we computed the results correctly we should see a nice horizontal bar chart of the counts per category below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to make a bar plot.\n",
    "plt.barh(res_q6[\"category\"], res_q6[\"total\"])\n",
    "plt.xlabel(\"Counts\")\n",
    "plt.ylabel(\"Job Category\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 7\n",
    "\n",
    "Now that we have a better sense of the basics of our data, we can ask some more interesting questions.\n",
    "\n",
    "The `Rating` table has the `numVotes` and the `averageRating` for each title. Which 10 films have the most ratings?\n",
    "\n",
    "Write a SQL query that outputs three fields: the `title`, `numVotes`, and `averageRating` for the 10 films that have the highest number of ratings.  Sort the result in descending order by the number of votes.\n",
    "\n",
    "**Hint**: The `numVotes` in the `Rating` table is not an integer! Use `CAST(Rating.numVotes AS int) AS numVotes` to convert the attribute to an integer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%sql imdb_engine --save query_q7\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# No further action is required. \n",
    "sql_q7 = %sqlcmd snippets query_q7\n",
    "res_q7 = pd.read_sql(sql_q7, imdb_mini_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Subpart 2: Election Donations in New York City\n",
    "\n",
    "Finally, let's analyze the Federal Election Commission (FEC)'s public records. We connect to the database using cell magic so that we can flexibly explore the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to connect to the fec_nyc database\n",
    "fec_nyc_db = 'duckdb:///' + str(path / 'fec_nyc.db')\n",
    "%sql duckdb:///../../../../../../../tmp/fec_nyc.db --alias fec_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table Descriptions\n",
    "\n",
    "Run the below cell to explore the **schemas** of all tables saved in the database.\n",
    "\n",
    "If you'd like, you can consult the below linked FEC pages for the descriptions of the tables themselves.\n",
    "\n",
    "* `cand` ([link](https://www.fec.gov/campaign-finance-data/candidate-summary-file-description/)): Candidates table. Contains names and party affiliation.\n",
    "* `comm` ([link](https://www.fec.gov/campaign-finance-data/committee-summary-file-description/)): Committees table. Contains committee names and types.\n",
    "* `indiv_sample_nyc` ([link](https://www.fec.gov/campaign-finance-data/contributions-individuals-file-description)): All individual contributions from New York City ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "/* just run this cell */\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  sqlite_master\n",
    "WHERE\n",
    "  type = 'table';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "Let's look at the `indiv_sample_nyc` table. The below cell displays individual donations made by residents of the state of New York. We use `LIMIT 5` to avoid loading and displaying a huge table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql fec_engine\n",
    "/* just run this cell */\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  indiv_sample_nyc\n",
    "LIMIT\n",
    "  5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql fec_engine\n",
    "/* just run this cell */\n",
    "SELECT\n",
    "  cand_id,\n",
    "  cand_name\n",
    "FROM\n",
    "  cand\n",
    "WHERE\n",
    "  cand_pty_affiliation = 'DEM'\n",
    "LIMIT\n",
    "  5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## [Tutorial] Matching Text with `LIKE`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's look at 2016 election contributions made by Donald Trump, who was a New York (NY) resident during that year. The following SQL query returns the `cmte_id`, `transaction_amt`, and `name` for every contribution made by any donor with \"DONALD\" and \"TRUMP\" in their name in the `indiv_sample_nyc` table.\n",
    "\n",
    "**Notes:**\n",
    "* We use the `WHERE ... LIKE '...'` to match fields with text patterns. The `%` wildcard represents at least zero characters. Compare this to what you know from regex!\n",
    "* We use `pd.read_sql` syntax here because we will do some EDA on the result `res`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see an example of LIKE.\n",
    "example_query = \"\"\"\n",
    "SELECT \n",
    "    cmte_id,\n",
    "    transaction_amt,\n",
    "    name\n",
    "FROM indiv_sample_nyc\n",
    "WHERE name LIKE '%TRUMP%' AND name LIKE '%DONALD%';\n",
    "\"\"\"\n",
    "\n",
    "example_res = pd.read_sql(example_query, fec_nyc_db)\n",
    "example_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the list above, it appears that some donations were not by Donald Trump himself, but instead by an entity called \"DONALD J TRUMP FOR PRESIDENT INC\". Fortunately, we see that our query only seems to have picked up one such anomalous name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to see the value counts for each candidate.\n",
    "example_res['name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 8\n",
    "\n",
    "\n",
    "\n",
    "In the cell below, revise the above query so that the 15 anomalous donations made by \"DONALD J TRUMP FOR PRESIDENT INC\" do not appear. Your resulting table should have 142 rows. \n",
    "\n",
    "**Hints:**\n",
    "* Consider using the above query as a starting point, or checking out the SQL query skeleton at the top of this lab. \n",
    "* The `NOT` keyword may also be useful here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%sql fec_engine --save query_q8\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for grading purposes. \n",
    "# No further action is required. \n",
    "sql_q8 = %sqlcmd snippets query_q8\n",
    "res_q8 = pd.read_sql(sql_q8, fec_nyc_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Print the number of rows in your query\n",
    "# Double check that this equals 142 \n",
    "res_q8.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 9: `JOIN`ing Tables\n",
    "\n",
    "Let's explore the other two tables in our database: `cand` and `comm`.\n",
    "\n",
    "The `cand` table contains summary financial information about each candidate registered with the FEC or appearing on an official state ballot for House, Senate or President."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql fec_engine\n",
    "/* just run this cell */\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  cand\n",
    "LIMIT\n",
    "  5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `comm` table contains summary financial information about each committee registered with the FEC. Committees are organizations that spend money for political action or parties, or spend money for or against political candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql fec_engine\n",
    "/* just run this cell */\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  comm\n",
    "LIMIT\n",
    "  5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "---\n",
    "\n",
    "#### Question 9a\n",
    "\n",
    "Notice that both the `cand` and `comm` tables have a `cand_id` column. Let's try joining these two tables on this column to print out committee information for candidates.\n",
    "\n",
    "List the first 5 candidate names (`cand_name`) in reverse lexicographic order (i.e reverse alphabetical order) by `cand_name`, along with their corresponding committee names. **Only select rows that have a matching `cand_id` in both tables.**\n",
    "\n",
    "Your output should look similar to the following:\n",
    "\n",
    "|cand_name|cmte_nm|\n",
    "|----|----|\n",
    "|ZUTLER, DANIEL PAUL MR|CITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT|\n",
    "|ZUMWALT, JAMES|ZUMWALT FOR CONGRESS|\n",
    "|...|...|\n",
    "\n",
    "Consider starting from the following query skeleton, which uses the `AS` keyword to rename the `cand` and `comm` tables to `c1` and `c2`, respectively.\n",
    "Which join is most appropriate?\n",
    "\n",
    "    SELECT ...\n",
    "    FROM cand AS c1\n",
    "        [INNER | {LEFT |RIGHT | FULL } {OUTER}] JOIN comm AS c2\n",
    "        ON ...\n",
    "    ...\n",
    "    ...;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%sql fec_engine --save query_q9a \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for grading purposes. \n",
    "# No further action is required. \n",
    "sql_q9a = %sqlcmd snippets query_q9a\n",
    "res_q9a = pd.read_sql(sql_q9a, fec_nyc_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "### Question 9b\n",
    "\n",
    "Suppose we modify the query from the previous part to include *all* candidates, **including those that don't have a committee.**\n",
    "\n",
    "\n",
    "List the first 5 candidate names (`cand_name`) in reverse lexicographic order by `cand_name`, along with their corresponding committee names. If the candidate has no committee in the `comm` table, then `cmte_nm` should be NULL (or `None` in the `python` representation).\n",
    "\n",
    "Your output should look similar to the following:\n",
    "\n",
    "|cand_name|cmte_nm|\n",
    "|----|----|\n",
    "|ZUTLER, DANIEL PAUL MR|CITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT|\n",
    "|...|...|\n",
    "|ZORNOW, TODD MR|None|\n",
    "\n",
    "**Hint**: Start from the same query skeleton as the previous part. \n",
    "Which join is most appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%sql fec_engine --save query_q9b\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for grading purposes. \n",
    "# No further action is required. \n",
    "sql_q9b = %sqlcmd snippets query_q9b\n",
    "res_q9b = pd.read_sql(sql_q9b, fec_nyc_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "## Question 10: Subqueries and Grouping\n",
    "\n",
    "If we return to our results from Question 8, we see that many of the contributions were to the same committee:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your SQL query result from Question 8\n",
    "# Reprinted for your convenience\n",
    "res_q8['cmte_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "For this question, create a new SQL query that returns the total amount that Donald Trump contributed to each committee.\n",
    "\n",
    "Your table should have four columns: `cmte_id`, `total_amount` (total amount contributed to that committee), `num_donations` (total number of donations), and `cmte_nm` (name of the committee). Your table should be sorted in **decreasing order** of `total_amount`.\n",
    "\n",
    "Your output should look similar to the following:\n",
    "\n",
    "|cmte_id|total_amount|num_donations|cmte_nm|\n",
    "|----|----|----|----|\n",
    "|C00580100|18633157|131|DONALD J. TRUMP FOR PRESIDENT, INC.|\n",
    "|C00055582|10000|1|NY REPUBLICAN FEDERAL CAMPAIGN COMMITTEE\n",
    "|...|...|...|\n",
    "\n",
    "**This is a hard question!** Don't be afraid to reference the lecture slides, or the overall SQL query skeleton at the top of this lab.\n",
    "\n",
    "**Hint**:\n",
    "\n",
    "* Note that committee names are not available in `indiv_sample_nyc`, so you will have to obtain information somehow from the `comm` table (perhaps a `JOIN` would be useful).\n",
    "* Remember that you can compute summary statistics after grouping by using aggregates like `COUNT(*)`, `SUM()` as output fields.\n",
    "* A **subquery** may be useful (but not required) to break your question down into subparts. Consider the following query skeleton, which uses the `WITH` operator to store a subquery's results in a temporary table named `donations`.\n",
    "\n",
    "        WITH donations AS (\n",
    "            SELECT ...\n",
    "            FROM ...\n",
    "            ... JOIN ...\n",
    "                ON ...\n",
    "            WHERE ...\n",
    "        )\n",
    "        SELECT ...\n",
    "        FROM donations\n",
    "        GROUP BY ...\n",
    "        ORDER BY ...;\n",
    "  \n",
    "**Note**: The video walkthrough solution may not be fully correct here. Remember that when using `GROUP BY`, all columns in the `SELECT` statement must either be present in the `GROUP BY` clause or be used in an aggregate function. For example:\n",
    "\n",
    "        SELECT titleType, SUM(runtimeMinutes), Year\n",
    "\t\tFROM Title\n",
    "\t\tGROUP BY titleType;\n",
    "\n",
    "Here, this query violates the rule because `Year` is included in the SELECT statement without being either part of an aggregate function or listed in the `GROUP BY` clause.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "%%sql fec_engine --save query_q10\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Run this cell for grading purposes. \n",
    "# No further action is required. \n",
    "sql_q10 = %sqlcmd snippets query_q10\n",
    "res_q10 = pd.read_sql(sql_q10, fec_nyc_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "finish",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/><br/>\n",
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />\n",
    "\n",
    "## Congratulations! You are finished with Lab 09!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://forms.gle/tUUekdzp91W4bxS87). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Submit this file to the Lab 09 assignment on Pensieve. If you run into any issues when running this cell, feel free to check this [section](https://ds100.org/debugging-guide/autograder_gradescope/autograder_gradescope.html#why-does-grader.exportrun_teststrue-fail-if-all-previous-tests-passed) in the Data 100 Debugging Guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False, run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q10": {
     "name": "q10",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res_q10.shape == (10, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sorted(res_q10['total_amount']) == [1000, 2000, 2600, 5000, 5000, 5200, 5400, 9000, 10000, 18633157]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sorted(res_q10['num_donations']) == [1, 1, 1, 1, 1, 1, 1, 2, 2, 131]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1a": {
     "name": "q1a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> marathon.shape == (50732, 4)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.isclose(true_prop, 0.24897500591342742))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1b": {
     "name": "q1b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(np.isclose(simulated_mean, true_prop, atol=0.01))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.isclose(simulated_var, true_prop * (1 - true_prop) / 100, atol=0.0001))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1c": {
     "name": "q1c",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(np.isclose(simulated_photos_mean, (true_prop + 3 * prop_dog_lover) * 100, atol=1))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.isclose(simulated_photos_var, 100 * true_prop * (1 - true_prop) + 900 * prop_dog_lover * (1 - prop_dog_lover), atol=5))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2a": {
     "name": "q2a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(np.isclose(np.mean(samples100), 0.24898, atol=0.01))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.isclose(np.mean(samples500), 0.24898, atol=0.01))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.isclose(np.mean(samples1000), 0.24898, atol=0.01))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2b": {
     "name": "q2b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(np.allclose(gaussian(0, 1, np.array([-1, 0, 1])), np.array([0.24197072, 0.39894228, 0.24197072])))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.allclose(gaussian(1, 2, np.array([-1, 0, 1])), np.array([0.10377687, 0.21969564, 0.28209479])))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3a": {
     "name": "q3a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(true_max == 50732)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3b": {
     "name": "q3b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(sample_maxes) == 5000\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.mean(sample_maxes) <= true_max)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(np.max(sample_maxes) <= true_max)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3d": {
     "name": "q3d",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> q3d in [1, 2, 3]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> q3d == 3\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4a": {
     "name": "q4a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(np.isclose(sample_correlation(data), scipy.stats.pearsonr(data['total_bill'], data['size'])[0]))\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4b": {
     "name": "q4b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(0.5 <= boot_ci[0] <= 0.52)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> bool(0.67 <= boot_ci[1] <= 0.69)\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res_q5.shape == (9, 1)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sorted(list(res_q5.iloc[:, 0])) == ['movie', 'short', 'tvEpisode', 'tvMiniSeries', 'tvMovie', 'tvSeries', 'tvSpecial', 'video', 'videoGame']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res_q6.shape == (12, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(res_q6['total']) == [21665, 13830, 12175, 11028, 6995, 4123, 2747, 1558, 623, 410, 66, 6]\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(res_q6['category']) == ['actor', 'writer', 'actress', 'producer', 'director', 'composer', 'cinematographer', 'editor', 'self', 'production_designer', 'archive_footage', 'archive_sound']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res_q7.shape == (10, 3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(res_q7.columns) == set(['title', 'numVotes', 'averageRating'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> np.sum(res_q7['numVotes'].astype(int)) == 19870486\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> list(res_q7['title']) == ['The Shawshank Redemption', 'The Dark Knight', 'Inception', 'Fight Club', 'Pulp Fiction', 'Forrest Gump', 'Game of Thrones', 'The Matrix', 'The Lord of the Rings: The Fellowship of the Ring', 'The Lord of the Rings: The Return of the King']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res_q8.shape == (142, 3)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sorted(res_q8['transaction_amt'].unique()) == [224, 514, 1000, 1029, 1216, 1713, 2000, 2426, 2460, 2529, 2531, 2536, 2544, 2574, 2587, 2600, 2620, 2679, 2692, 2693, 2700, 2991, 3063, 3347, 3360, 3371, 3499, 3548, 3706, 3954, 4049, 4127, 4164, 4591, 4931, 4958, 5000, 5297, 5574, 6197, 6250, 6990, 7401, 7519, 7990, 8735, 8896, 9000, 9013, 9106, 9651, 9752, 10000, 10418, 11023, 15000, 23775, 2000000, 10000000]\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9a": {
     "name": "q9a",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res_q9a.shape == (5, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(res_q9a == res_q9a.sort_values('cand_name', ascending=False))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sorted(res_q9a['cmte_nm'].unique()) == ['CITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT', 'CONSTITUTIONAL COMMITTEE', 'JOE ZUCCOLO FOR CONGRESS', 'ZUKOWSKI FOR CONGRESS', 'ZUMWALT FOR CONGRESS']\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9b": {
     "name": "q9b",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> res_q9b.shape == (5, 2)\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> all(res_q9b == res_q9b.sort_values('cand_name', ascending=False))\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> set(res_q9b['cand_name']) == set(['ZUTLER, DANIEL PAUL MR', 'ZUMWALT, JAMES', 'ZUKOWSKI, ANDREW GEORGE', 'ZUCCOLO, JOE', 'ZORNOW, TODD MR'])\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> sorted(map(str, res_q9b['cmte_nm'].unique())) == ['CITIZENS TO ELECT DANIEL P ZUTLER FOR PRESIDENT', 'JOE ZUCCOLO FOR CONGRESS', 'None', 'ZUKOWSKI FOR CONGRESS', 'ZUMWALT FOR CONGRESS']\nTrue",
         "hidden": false,
         "locked": false
        },
        {
         "code": ">>> res_q9b['cmte_nm'][4] == None\nTrue",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
