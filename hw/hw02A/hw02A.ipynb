{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"hw02A.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "intro-hw2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# üçï Homework 2A: Food Safety\n",
    "\n",
    "## üêº Cleaning and Exploring Data with Pandas\n",
    "\n",
    "## ‚ÄºÔ∏è Due Date: Wednesday, July 2nd, 11:59 PM\n",
    "You must submit this assignment to Pensieve by the on-time deadline, Wednesday, July 2nd, 11:59 PM. Please read the syllabus for the [Slip Day policy](https://ds100.org/su25/syllabus/#:~:text=is%20considered%20late.-,Slip%20Days,-Each%20student%20gets). No late submissions beyond the details in the Slip Day policy will be accepted. **We strongly encourage you to plan to submit your work to Pensieve several hours before the stated deadline.** This way, you will have ample time to reach out to staff for support if you encounter difficulties with submission. While course staff is happy to help guide you with submitting your assignment ahead of the deadline, we will not respond to last-minute requests for assistance (TAs need to sleep, after all!).\n",
    "\n",
    "Please read the instructions carefully when you are submitting your work to Pensieve.\n",
    "\n",
    "## üí™ Collaboration Policy\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others, please **include their names** below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators**: *list collaborators here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## üìú This Assignment\n",
    "\n",
    "In this homework, we will investigate restaurant food safety scores for restaurants in San Francisco. The scores and violation information have been [made available by the San Francisco Department of Public Health](https://data.sfgov.org/Health-and-Social-Services/Restaurant-Scores-LIVES-Standard/pyih-qa8i). The main goal for this assignment is to walk through the process of Data Cleaning and familiarise yourself with some of the `pandas` functions discussed in Pandas I and II. \n",
    "\n",
    "\n",
    "After this homework, you should be comfortable with:\n",
    "* Reading CSV files, \n",
    "* Reading `pandas` documentation and using `pandas`,\n",
    "* Working with data at different levels of granularity, and\n",
    "* Identifying the type of data collected, missing values, anomalies, etc., and doing some basic analysis.\n",
    "\n",
    "## üíØ Score Breakdown \n",
    "Question | Manual | Points\n",
    "--- | --- | ---\n",
    "1a | no | 1\n",
    "1b | no | 2\n",
    "1c | no | 1\n",
    "2a | no | 1\n",
    "2bi | no | 1\n",
    "2bii | no | 1\n",
    "2ci | no | 1\n",
    "2cii | no | 1\n",
    "2d | yes | 1\n",
    "2e | no | 2\n",
    "2f | no | 1\n",
    "3a | no | 1\n",
    "3b | no | 2\n",
    "3ci | no | 1\n",
    "3cii | no | 1\n",
    "3ciii | no | 1\n",
    "3civ | no | 1\n",
    "4a | no | 1\n",
    "4b | no | 1\n",
    "4c | no | 1\n",
    "5a | no | 2\n",
    "5b | no | 2\n",
    "Total | | 27\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úä Before You Start\n",
    "\n",
    "For each question in the assignment, please write down your answer in the answer cell(s) right below the question. \n",
    "\n",
    "We understand that it is helpful to have extra cells breaking down the process towards reaching your final answer. If you happen to create new cells below your answer to run codes, **NEVER** add cells between a question cell and the answer cell below it. It will cause errors when we run the autograder, and it will sometimes cause a failure to generate the PDF file.\n",
    "\n",
    "**Important note: The local autograder tests will not be comprehensive. You can pass the automated tests in your notebook but still fail tests in the autograder.** Please be sure to check your results carefully.\n",
    "\n",
    "Finally, unless we state otherwise, **do not use for loops or list comprehensions**. The majority of this assignment can be done using built-in commands in `pandas` and `NumPy`. Please also don't hard code anything. Our autograder isn't smart enough to check, but you're depriving yourself of key learning objectives if you write loops or comprehensions, and you also won't be ready for the midterm.\n",
    "\n",
    "The cell below imports all the necessary libraries you need to use during this homework. Without running this cell, you will not be able to call the various `NumPy` and `pandas` functions we use later on, so please make sure you run it before starting to work on the homework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from IPython.display import display, Image\n",
    "def display_figure_for_grader(fig):\n",
    "    plotly.io.write_image(fig, 'temp.png')\n",
    "    display(Image('temp.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÄ 0: Obtaining the Data\n",
    "\n",
    "## File Systems and I/O\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we will focus on using Python commands to investigate files. However, it can sometimes be easier to use shell commands in your local operating system. The following cells demonstrate how to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path('.') # Create a Path object to the current working directory\n",
    "data_dir.mkdir(exist_ok = True) # Create a directory\n",
    "file_path = data_dir / Path('data.zip') # Create a path to the .zip file that contains our datasets\n",
    "dest_path = file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the cell above, if you list the contents of the directory containing this notebook, you should see `data.zip`.\n",
    "\n",
    "*Note*: The command below starts with an `!`. This tells our Jupyter Notebook to pass this command to the operating system. In this case, the command is the `ls` Unix command, which lists files in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Food Safety Data\n",
    "\n",
    "We have data, but we don't have any specific questions about the data yet. Let's focus on understanding the structure of the data; this involves answering questions such as:\n",
    "\n",
    "* Is the data in a standard format or encoding?\n",
    "* Is the data organized in records?\n",
    "* What are the fields in each record? (We sometimes also use the term 'feature' or 'attribute' as well, depending on the context)\n",
    "\n",
    "Let's start by looking at the contents of `data.zip`. It's not just a single file but rather a compressed directory of multiple files. We could inspect it by uncompressing it using a shell command such as `!unzip data.zip`, but in this homework, we're going to do almost everything in Python for maximum portability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Inside and Extracting the Zip Files\n",
    "\n",
    "The following code blocks are for setup. Simply run the cells; **do not modify them**. Question 1a is where you will start to write code.\n",
    "\n",
    "Here, we assign `my_zip` to a `zipfile.Zipfile` object representing `data.zip`, and assign `list_names` to a list of all the names of the contents in `data.zip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "my_zip = zipfile.ZipFile(dest_path, 'r') # Read .zip file at dest_path, which is data.zip\n",
    "list_names = my_zip.namelist() # Create list of names of contens in data.zip\n",
    "list_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that we did not write `zipfile.ZipFile('data.zip', ...)`. Instead, we used `zipfile.ZipFile(dest_path, ...)`. In general, we **strongly suggest having your filenames hard coded as string literals only once** in a notebook. It is very dangerous to hardcode things twice because if you change one but forget to change the other, you can end up with bugs that are very hard to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we display the files' names and their sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_zip = zipfile.ZipFile(dest_path, 'r') # Read .zip file at dest_path, which is data.zip\n",
    "for info in my_zip.infolist(): # Extract information about contents of .zip file, iterate through them\n",
    "    print('{}\\t{}'.format(info.filename, info.file_size)) # Print file name and uncompressed file size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often when working with zipped data, we'll never unzip the actual zip file. This saves space on our local computer. However, for this homework the files are small, so we're just going to unzip everything. This has the added benefit that you can look inside the CSV files using a text editor, which might be handy for understanding the structure of the files. The cell below will unzip the CSV files into a sub-directory called `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('.') # Create a path to current working directory\n",
    "my_zip.extractall(data_dir) # Extract all contents from data.zip\n",
    "!ls {data_dir / Path(\"data\")} # List out contents of \"data\" sub-directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above created a folder called `data`, and in it, there should be five CSV files. Let's open up `legend.csv` to see its contents. To do this, click on the file icon on the top left to show the folders and files within the hw02A folder, then click `legend.csv`. The file will open up in another tab. You should see something that looks like this:\n",
    "\n",
    "    \"Minimum_Score\",\"Maximum_Score\",\"Description\"\n",
    "    0,70,\"Poor\"\n",
    "    71,85,\"Needs Improvement\"\n",
    "    86,90,\"Adequate\"\n",
    "    91,100,\"Good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `legend.csv` file does indeed look like a well-formed CSV file. Let's check the other three files. Rather than opening up each file manually, let's use Python to print out the first 5 lines of each. We defined a helper function for you that will allow you to retrieve the first N lines of a file as a list. For example, `head('data/legend.csv', 5)` will return the first 5 lines of `data/legend.csv.gz`. Run the cell below to print out the first 5 lines of all six files that we just extracted from the zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def head(filename, lines=5):\n",
    "    \"\"\"\n",
    "    Returns the first few lines of a file.\n",
    "\n",
    "    filename: the name of the file to open\n",
    "    lines: the number of lines to include\n",
    "\n",
    "    return: A list of the first few lines from the file.\n",
    "    \"\"\"\n",
    "    from itertools import islice\n",
    "    with open(filename, \"r\") as f: # Open file for reading\n",
    "        return list(islice(f, lines)) # Return first five lines of the file as a list\n",
    "\n",
    "data_dir = \"./\" # Path to current working directory\n",
    "for f in list_names: # Iterate through names of contents in \"data\" sub-directory\n",
    "    if not os.path.isdir(f): # Check if specified path is a directory\n",
    "        print(head(data_dir + f, 5), \"\\n\") # Create path to respective file in \"data\" sub-directory, use `head` function defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in and Verifying Data\n",
    "\n",
    "Based on the above information, let's attempt to load `bus.csv`, `ins2vio.csv`, `ins.csv`, and `vio.csv` into `pandas` `DataFrame`s with the following names: `bus`, `ins2vio`, `ins`, and `vio`, respectively.\n",
    "\n",
    "*Note:* Because of character encoding issues, one of the files (`bus`) will require an additional argument `encoding='ISO-8859-1'` when calling `pd.read_csv`. At some point in your future, you should read all about [character encodings](https://diveintopython3.problemsolving.io/strings.html). We won't discuss these in detail in Data 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing data\n",
    "dsDir = Path('data')\n",
    "\n",
    "bus = pd.read_csv(dsDir/'bus.csv', encoding='ISO-8859-1')\n",
    "ins2vio = pd.read_csv(dsDir/'ins2vio.csv')\n",
    "ins = pd.read_csv(dsDir/'ins.csv')\n",
    "vio = pd.read_csv(dsDir/'vio.csv')\n",
    "\n",
    "# This code is essential for the autograder to function properly. Do not edit\n",
    "bus.loc[:, [\"latitude\", \"longitude\"]] = bus.loc[:, [\"latitude\", \"longitude\"]].apply(lambda x: np.round(x, 2))\n",
    "ins_test = ins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've read the files let's try some `pd.DataFrame` methods ([docs](https://pandas.pydata.org/pandas-docs/version/1.4.3/reference/api/pandas.DataFrame.html)).\n",
    "Use the `DataFrame.head` method to show the top few lines of the `bus`, `ins`, and `vio` `DataFrame`s. For example, running the cell below will display the first few lines of the `bus` `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show multiple return outputs in one single cell, you can use¬†`display()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(bus.head())\n",
    "display(ins.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DataFrame.describe` method can also be handy for computing summaries of numeric columns of our `DataFrame`s. Try it out with each of our 4 `DataFrame`s. Below, we have used the method to give a summary of the `bus` `DataFrame`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform some sanity checks for you to verify that the data was loaded with the correct structure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check the basic structure of the `DataFrame`s you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert all(bus.columns == ['business id column', 'name', 'address', 'city', 'state', 'postal_code',\n",
    "                           'latitude', 'longitude', 'phone_number']) # Check that the `bus` DataFrame contains the right columns in the correct order\n",
    "assert 6250 <= len(bus) <= 6260 # Check that the `bus` DataFrame has the correct number of rows\n",
    "\n",
    "assert all(ins.columns == ['iid', 'date', 'score', 'type']) # Similar to above, check that the `ins` DataFrame contains the right columns in order\n",
    "assert 26660 <= len(ins) <= 26670 # Similar to above, check that the `ins` DataFrame has the correct number of rows\n",
    "\n",
    "assert all(vio.columns == ['description', 'risk_category', 'vid'])\n",
    "assert 60 <= len(vio) <= 65\n",
    "\n",
    "assert all(ins2vio.columns == ['iid', 'vid'])\n",
    "assert 40210 <= len(ins2vio) <= 40220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fun Fact*: The series of `assert` statements above is similar to how course staff implement test cases for assignments! Next time you fail a public test case on an assignment, take a look at the error message and you will probably find the `assert` statement which is causing you to fail the test case.\n",
    "\n",
    "Next we'll check that the statistics match what we expect. The following are hard-coded statistical summaries of the correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_summary = pd.DataFrame(**{'columns': ['business id column', 'latitude', 'longitude'],\n",
    " 'data': {'business id column': {'50%': 75685.0, 'max': 102705.0, 'min': 19.0},\n",
    "  'latitude': {'50%': -9999.0, 'max': 37.82, 'min': -9999.0},\n",
    "  'longitude': {'50%': -9999.0,\n",
    "   'max': 0.0,\n",
    "   'min': -9999.0}},\n",
    " 'index': ['min', '50%', 'max']})\n",
    "\n",
    "ins_summary = pd.DataFrame(**{'columns': ['score'],\n",
    " 'data': {'score': {'50%': 76.0, 'max': 100.0, 'min': -1.0}},\n",
    " 'index': ['min', '50%', 'max']})\n",
    "\n",
    "vio_summary = pd.DataFrame(**{'columns': ['vid'],\n",
    " 'data': {'vid': {'50%': 103135.0, 'max': 103177.0, 'min': 103102.0}},\n",
    " 'index': ['min', '50%', 'max']})\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "print('What we expect from your Businesses DataFrame:')\n",
    "display(bus_summary)\n",
    "print('What we expect from your Inspections DataFrame:')\n",
    "display(ins_summary)\n",
    "print('What we expect from your Violations DataFrame:')\n",
    "display(vio_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a testing function that we'll use to verify that your data has the same statistics as what we expect. Run these cells to define the function. The `df_allclose` function has this name because we are verifying that all of the statistics for your `DataFrame` are close to the expected values. Why not `df_allequal`? It's a bad idea in almost all cases to compare two floating point values like 37.780435, as rounding errors can cause spurious failures. Run the following cells to load some basic utilities (you do not need to change these at all):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run this cell to load this utility comparison function that we will use in various\n",
    "tests below (both tests you can see and those we run internally for grading).\n",
    "\n",
    "Do not modify the function in any way.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def df_allclose(actual, desired, columns=None, rtol=5e-2):\n",
    "    \"\"\"Compare selected columns of two Dataframes on a few summary statistics.\n",
    "\n",
    "    Compute the min, median, and max of the two Dataframes on the given columns, and compare\n",
    "    that they match numerically to the given relative tolerance.\n",
    "\n",
    "    If they don't match, an AssertionError is raised (by `numpy.testing`).\n",
    "    \"\"\"\n",
    "    # Summary statistics to compare on\n",
    "    stats = ['min', '50%', 'max']\n",
    "\n",
    "    # For the desired values, we can provide a full DF with the same structure as\n",
    "    # the actual data or pre-computed summary statistics.\n",
    "    # We assume a pre-computed summary was provided if a column is None. In that case,\n",
    "    # `desired` *must* have the same structure as the actual's summary\n",
    "    if columns is None:\n",
    "        des = desired\n",
    "        columns = desired.columns\n",
    "    else:\n",
    "        des = desired[columns].describe().loc[stats]\n",
    "\n",
    "    # Extract summary stats from actual DF\n",
    "    act = actual[columns].describe().loc[stats]\n",
    "\n",
    "    return np.allclose(act, des, rtol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore each file in turn, including determining its granularity and exploring many of the variables individually. Let's begin with the businesses file, which has been read into the `bus` `DataFrame`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "business-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    },
    "tags": []
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "# üìö 1: Examining the Business Data File\n",
    "\n",
    "## üöå Question 1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From its name alone, we expect the `bus.csv` file to contain information about the restaurants. Let's investigate the granularity of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `bus` `DataFrame` contains a column called `business id column`, which probably corresponds to a unique business id.  However, we will first rename that column to `bid` for simplicity.\n",
    "\n",
    "**Note**: In practice, we might want to do this renaming when the table is loaded, but for grading purposes, we will do it here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = bus.rename(columns={\"business id column\": \"bid\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Examining the entries in `bus`, is the `bid` unique for each record (i.e., each row of data)? Your code should compute the answer, i.e., don't just hard code `True` or `False`.\n",
    "\n",
    "**Hint**: Use `value_counts()` (documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html)) or `unique()` (documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.unique.html)) to determine if the `bid` series has any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_bid_unique = len(bus['bid'].unique()) == len(bus['bid'])\n",
    "# is_bid_unique = bus['bid'].is_unique\n",
    "is_bid_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q1a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üöå Question 1b\n",
    "\n",
    "We will now work with some important fields in `bus`.\n",
    "\n",
    "1. Assign `top_names` to a `NumPy` array or list containing the top 7 most frequently used business names, from most frequent to least frequent. \n",
    "2. Assign `top_addresses` to a `NumPy` array or list containing the top 7 addresses where businesses are located, from most popular to least popular.\n",
    "\n",
    "**Hint 1**: You may find `pd.Series.value_counts` helpful (documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.value_counts.html)).\n",
    "\n",
    "**Hint 2**: You'll need to get the names and addresses, NOT the counts associated with each. Some way to **reset the index** would come in handy. If you're unsure how to do this, try looking through the class notes or using a search engine. Part of the goal of this course is to develop independent thinking in the context of the data science lifecycle, which can involve a fair bit of exploring and reading documentation. It may be a bit annoying at first, but you‚Äôll get the hang of it. We‚Äôre here to help guide you on that path if you ever need support!\n",
    "\n",
    "**Hint 3**: To check your answer, `top_names[0]` should return the string `Peet's Coffee & Tea`. It should not be a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_names = np.array(bus.value_counts('name').head(7).index)\n",
    "top_addresses = np.array(bus.value_counts('address').head(7).index)\n",
    "\n",
    "display(top_names)\n",
    "display(top_addresses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q1b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## üöå Question 1c\n",
    "\n",
    "Based on the above exploration, what does each record in the `bus` `DataFrame` represent?\n",
    "\n",
    "**A**. A city block.\n",
    "\n",
    "**B**. One location of a restaurant.\n",
    "\n",
    "**C**. A chain of restaurants.\n",
    "\n",
    "\n",
    "\n",
    "Answer in the following cell. Your answer should be a string, either `\"A\"`, `\"B\"`, or `\"C\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1c = \"B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q1c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "business-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "# üßπ 2: Cleaning the Business Data Postal Codes\n",
    "\n",
    "The business data contains postal code information that we can use to aggregate the ratings over regions of the city. Let's examine and clean the postal code field. The postal code (sometimes also called a [ZIP code](https://en.wikipedia.org/wiki/ZIP_Code)) partitions the city into regions:\n",
    "\n",
    "<img src=\"https://gisgeography.com/wp-content/uploads/2023/07/San-Francisco-Zip-Code-Map-1-2048x2048.jpg\" alt=\"ZIP Code Map\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## ü•° Question 2a\n",
    "\n",
    "How many restaurants are in each ZIP code? \n",
    "\n",
    "In the cell below, create a **Series** where the index is the postal code, and the value is the number of records with that postal code. The `Series` should be in descending order of count. Do you notice any odd/invalid ZIP codes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_counts = bus.groupby('postal_code').size().sort_index()\n",
    "print(zip_counts.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q2a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "\n",
    "## ü•° Question 2b\n",
    "\n",
    "In Question 2a, we noticed a large number of potentially invalid ZIP codes (e.g., \"Ca\"). These are likely due to data entry errors. To get a better understanding of the potential errors in the zip codes, let's break down the problem into two parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### üöÄ Part I\n",
    "\n",
    "Import a list of valid San Francisco ZIP codes by using `pd.read_json` to load the file `data/sf_zipcodes.json`, and store them as a **Series** in `valid_zip_codes`. As you may expect, `pd.read_json` works similarly to `pd.read_csv` but for JSON files (a different file format you'll learn more about in HW 3) that you can read more about [here](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html). **Make sure that the resulting series (not a data frame!) is zero-indexed, with each value being a unique zip code.** If you are unsure of what data type a variable is, remember you can do `type(some_var_name)` to check!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_zip_codes = pd.read_json(dsDir/'sf_zipcodes.json')[\"zip_codes\"]\n",
    "valid_zip_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q2bi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that `pd.read_json` reads data as integers by default. This isn't quite what we want! We would like to store ZIP codes as strings (you'll learn more about why soon!). To do that, we can use the `astype` function to generate a copy of the `pandas` `Series` stored as strings instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_zip_codes = valid_zip_codes.astype(\"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're ever unsure about the data type of a variable, remember you can always check using the `type` function like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_zip_codes.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to do Part II. You will probably want to use the `Series.isin` function. For more information on this function see the [documentation linked in this internet search](https://www.google.com/search?q=series+isin+pandas&rlz=1C1CHBF_enUS910US910&oq=series+isin+pandas&aqs=chrome..69i57l2j69i59j69i60l2j69i65j69i60l2.1252j0j7&sourceid=chrome&ie=UTF-8). \n",
    "\n",
    "**Note:** You are welcome and, in fact, encouraged to search and read the documentation on the internet to complete the assignments in the course, even if the documentation is not linked explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### üöÄ Part II\n",
    "\n",
    " Construct a `DataFrame` containing only the businesses that **DO NOT** have valid ZIP codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_zip_bus = bus[~bus['postal_code'].isin(valid_zip_codes)]\n",
    "invalid_zip_bus.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q2bii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "--- \n",
    "\n",
    "## ‚ùì Question 2c\n",
    "\n",
    "In the previous question, many of the businesses had a common invalid postal code that was likely used to encode a MISSING postal code. Do they all share a potentially \"interesting address\"? For that purpose, in the following cells, we will construct a series that counts the number of businesses at each `address` that have this single likely MISSING postal code value. \n",
    "\n",
    "Let's break this down into steps: \n",
    "\n",
    "### ‚ùì Part 1\n",
    "Identify the single common missing postal code and assign it to `missing_postal_code`. Then create a `DataFrame`, `bus_missing`, to store only those businesses in `bus` that have `missing_postal_code` as their postal code.\n",
    "\n",
    "**Hint**: All ZIP codes in the US are *positive* numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_postal_code = '-9999'\n",
    "bus_missing = bus[bus['postal_code'] == missing_postal_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q2ci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚ùì Part 2\n",
    "Using `bus_missing`, find the number of businesses at each address (which would all share the same postal code). Specifically, `missing_zip_address_count` should store a `Series` with addresses as the indices and the counts as the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_zip_address_count = bus_missing.groupby('address').size().sort_values(ascending=False)\n",
    "missing_zip_address_count.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q2cii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "--- \n",
    "\n",
    "## ‚ùì Question 2d\n",
    "\n",
    "**(Written Response)** If we were to drop businesses with postal code values equal to `missing_postal_code`, what **specific types of businesses** would we be excluding? In other words, is there a commonality among businesses with missing postal codes? **Please respond in no more than 2 sentences.**\n",
    "\n",
    "**Hint**: You may want to identify and Google the names of the businesses with missing postal codes. Feel free to reuse parts of your code from 2c to re-examine `bus_missing` but we will not be grading your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "otter_answer_cell"
    ]
   },
   "source": [
    "street food vendors, or food trucks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "--- \n",
    "\n",
    "## ‚ùì Question 2e\n",
    "\n",
    "Examine the `invalid_zip_bus` `DataFrame` we computed in Question 2c and look at the businesses that DO NOT have the special MISSING ZIP code value. Some of the invalid postal codes are just the full 9-digit code rather than the first 5 digits. **Create a new column named `postal5` in the original `bus` `DataFrame`, which contains only the first 5 digits of the `postal_code` column.**\n",
    "\n",
    "Then, for any of the `postal5` ZIP code entries that were not a valid San Francisco ZIP code (according to `valid_zip_codes`), the provided code will set the `postal5` value to `None`. \n",
    "\n",
    "**Hint:** You will find `str` accessors particularly useful. They allow you to use your usual Python string functions in tandem with a `DataFrame`. Refer to the [Pandas II course notes](https://ds100.org/course-notes/pandas_2/pandas_2.html#:~:text=babyname_lengths%20%3D%20babynames%5B%22Name%22%5D.str.len()) for examples.\n",
    "\n",
    "**Do not modify the provided code! Simply add your own code in place of the ellipses.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus['postal5'] = None\n",
    "bus['postal5'] = bus['postal_code'].str.slice(0, 5)\n",
    "\n",
    "bus.loc[~bus['postal5'].isin(valid_zip_codes), 'postal5'] = None\n",
    "# Checking the corrected postal5 column\n",
    "bus.loc[invalid_zip_bus.index, ['bid', 'name', 'postal_code', 'postal5']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q2e\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "## ‚ùì Question 2f\n",
    "\n",
    "Finally, use the `postal5` column to create a `DataFrame`, `bus_valid`, that only contains the rows of `bus` where a `postal5` zip code exists. You may find the `.isna()` function useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_valid = bus[~bus['postal5'].isna()]\n",
    "bus_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q2f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "business-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "# üîç 3: Investigate the Inspection Data\n",
    "\n",
    "Let's now turn to the inspection `DataFrame`. Earlier, we found that `ins` has 4 columns named \n",
    "`iid`, `score`, `date`, and `type`.  In this section, we determine the granularity of `ins` and investigate the kinds of information provided for the inspections. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-174ed23c543ad9da",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's start by looking again at the first 5 rows of `ins` to see what we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Question 3a\n",
    "\n",
    "The column `iid` probably corresponds to an inspection ID. Write an expression (i.e., a line of code) that evaluates to `True` or `False` based on whether all the inspection IDs are unique. Your code should compute the answer, i.e., don't just hard code `True` or `False`.\n",
    "\n",
    "**Hint:** This is a very similar question to Question 1a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_ins_iid_unique = ins['iid'].is_unique\n",
    "is_ins_iid_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q3a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Question 3b\n",
    "\n",
    "We want to extract `bid` from each row of the `ins` `DataFrame`. If we look carefully, the column `iid` of the `ins` `DataFrame` appears to be composed of two numbers, and the first number looks like a business ID.  \n",
    "\n",
    "Create a new column called `bid` in the `ins` Dataframe containing just the business ID. You will want to use `ins['iid'].str` operations. (Python's in-built `split` method could come in use; read up on the documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html)!) Also, be sure to convert the type of this column to `int`. \n",
    "\n",
    "**Hint**: Similar to an earlier problem where we used `astype(\"string\")` to convert a column to a string, here you should use `astype` to convert the `bid` column into type `int`. **No Python `for` loops or list comprehensions are allowed.** This is on the honor system since our autograder isn't smart enough to check, but if you're using `for` loops or list comprehensions, you're doing the HW incorrectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins['bid'] = ins['iid'].str.split('_').str[0].astype(int)\n",
    "ins.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q3b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Question 3c\n",
    "\n",
    "For this part, we're going to explore some new, somewhat strange syntax that we haven't seen in the lecture. Don't panic! If you're not sure what to do, try experimenting or Googling, and don't shy away from talking to other students or course staff.\n",
    "\n",
    "For this problem, we'll use the time component of the inspection data.  All this information is given in the `date` column of the `ins` `DataFrame`. \n",
    "\n",
    "**No Python `for` loops or list comprehensions are allowed!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Part I\n",
    "\n",
    "What is the type of the individual `ins['date']` entries? You may want to grab the very first entry and use the `type` function in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins_date_type = type(ins['date'][0])\n",
    "ins_date_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q3ci\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### üîç Part II\n",
    "Rather than the type you discovered in Part 1, we want each entry in `pd.TimeStamp` format. You might expect that the usual way to convert something from its current type to `TimeStamp` would be to use `astype`. You can do that, but the more typical way is to use `pd.to_datetime` (documentation [here](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)). Using `pd.to_datetime`, create a new `ins['timestamp']` column containing `pd.Timestamp` objects. These will allow us to do date manipulation with much greater ease in parts III and IV. \n",
    "\n",
    "**Note:** You may encounter a `UserWarning` if you do not specify the date format when using pd.to_datetime. To resolve this, consider specifying the `format` using the string `'%m/%d/%Y %I:%M:%S %p'`. This format indicates that the datetime we are inputting to convert is in the form of `Month`/`Day`/`Year` `Hour` (12-hour clock)/`Minute`/`Second` `AM`/`PM`. Read [here](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_string = '%m/%d/%Y %I:%M:%S %p'\n",
    "ins['timestamp'] = pd.to_datetime(ins['date'], format=format_string)\n",
    "ins['timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q3cii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### üîç Part III\n",
    "\n",
    "What are the earliest and latest dates in our inspection data?  \n",
    "\n",
    "**Hint**: you can use `min` and `max` on dates of the correct type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earliest_date = ins['timestamp'].min()\n",
    "latest_date = ins['timestamp'].max()\n",
    "print(\"Earliest Date:\", earliest_date)\n",
    "print(\"Latest Date:\", latest_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q3ciii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### üîç Part IV\n",
    "\n",
    "We probably want to examine the inspections by year. Create an additional `ins['year']` column containing just the year of the inspection. Consider using `pd.Series.dt.year` to do this.\n",
    "\n",
    "In case you're curious, the documentation for `TimeStamp` data can be found at [this link](https://pandas.pydata.org/docs/reference/api/pandas.Timestamp.html#pandas.Timestamp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins['year'] = ins['timestamp'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q3civ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "<br/>\n",
    "\n",
    "# üßë‚Äç‚öñÔ∏è 4: Benford's Law\n",
    "\n",
    "[Benford‚Äôs Law](https://en.wikipedia.org/wiki/Benford%27s_law), also known as the Law of Anomalous Numbers or the First Digit Law, is a phenomenon seen in many real-world datasets. In datasets where Benford‚Äôs Law applies, there is an overrepresentation of small numbers (e.g., 1 and 2) in the leading digits of numeric entries and an underrepresentation of large numbers (e.g., 8 and 9). Benford‚Äôs Law is observed in river lengths, national populations, and has even been used to investigate [financial fraud](https://www.acfe.com/acfe-insights-blog/blog-detail?s=what-is-benfords-law-and-why-fraud-examiners-use-it)! More about Benford‚Äôs Law can be found [here](https://www.scientificamerican.com/article/what-is-benfords-law-why-this-unexpected-pattern-of-numbers-is-everywhere/).\n",
    "\n",
    "<img src=\"https://static.scientificamerican.com/sciam/assets/File/BenfordsLaw_graphic_d%5B68%5D.jpg?w=1350\" alt=\"Benford's Law Distribution\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Let‚Äôs investigate whether or not the **address** numbers in the `bus_valid` `DataFrame` follow Benford‚Äôs Law."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## üßë‚Äç‚öñÔ∏è Question 4a\n",
    "\n",
    "First, create a new column, `first_char` that contains the first character of the address string of each restaurant in `bus_valid`. **Again, do not use for loops or list comprehensions in your solution**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_valid[\"first_char\"] = bus_valid[\"address\"].str[0]\n",
    "bus_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q4a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äç‚öñÔ∏è Question 4b\n",
    "\n",
    "Unfortunately, not all of the addresses in `bus_valid` start with a number. Create a new `DataFrame` `bus_digits` that only contains the rows of `bus_valid` where the restaurant's address starts with a digit between 1 and 9.\n",
    "\n",
    "**Note: Do not include rows in which the first digit of the address is a 0.**\n",
    "\n",
    "*Hint*: You may find the `s.str.isnumeric()` `Series` method useful here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(pd.Series.str.isnumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_digits = bus_valid[bus_valid['first_char'].str.isdigit()]\n",
    "bus_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q4b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äç‚öñÔ∏è Question 4c\n",
    "\n",
    "Plot a histogram that shows the frequency of each possible first digit among the addresses in `bus_digits`. Like you did in Homework 1, create bins of unit length that are centered at each of the digits and assign it to `bins_arr`.\n",
    "\n",
    "**Note**: You only need to define `digits_arr` and `bins_arr`. The code necessary to plot the histogram has already been provided for you.\n",
    "\n",
    "**Hint**: Make sure the `digits_array` has data type `int`. You can check the type of the object in a `Series` by looking at the `dtype` attribute. More information [here](https://pandas.pydata.org/docs/reference/api/pandas.Series.dtype.html). How can you change the content of the series to a specific data type?\n",
    "\n",
    "**Hint:** To create bins of unit length that are centered at each of the digits, you might not want to start at the original starting and ending points. Where should you start instead?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits_arr = bus_digits['first_char'].astype(int)\n",
    "bins_arr = np.arange(round(digits_arr.min()) - 0.5, round(digits_arr.max()) + 0.5)\n",
    "\n",
    "# DO NOT CHANGE BELOW\n",
    "benford = np.array([0.301, 0.176, 0.125, 0.097, 0.079, 0.067, 0.058, 0.051, 0.046]) # Array of proportions under Benford's Law\n",
    "plt.hist(digits_arr, bins = bins_arr, density = True, alpha = 0.5, color = \"blue\", label = \"Empirical Dist\") # Plot digits_arr, the empirical distribution\n",
    "plt.bar(np.arange(1, 10), height = benford, width = 1, alpha = 0.5, color = \"red\", label = \"Benford's Law\") # Plot the theoretical distribution under Benford's Law\n",
    "plt.xlabel(\"Digit\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(\"Overlaid Plot of Empirical Distribution vs Benford's Law\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q4c\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have worked extensively with postal codes in this homework assignment. As prevalent and fascinating as it is, Benford's Law does not apply to *any* dataset. Benford's Law applies to datasets in which the numbers are *naturally generated*, which means that we will not be able to observe the same phenomenon in the leading digits of postal codes across the United States which are *manually assigned*. \n",
    "\n",
    "<img src=\"https://www.unitedstateszipcodes.org/images/zip-codes/zip-codes.png\" alt=\"Leading Digit of ZIP Code across US\" style=\"width: 600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üèòÔ∏è 5: Search for the Closest Restaurant\n",
    "\n",
    "Conan plans to go on a weekend trip to San Francisco, where he hopes to grab some food near Fisherman‚Äôs Wharf. He believes that the closer he is to the area the better the food will be, so he chooses a restaurant solely by its coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## üèòÔ∏è Question 5a\n",
    "\n",
    "Create a new `DataFrame` `bus_coords`, which includes only the rows of `bus_valid` where both the latitude and longitude values are not missing. \n",
    "\n",
    "**Hint:** Missing values for latitude and longitude values are the *same* as the most frequent missing value you found in **Question 2c**!\n",
    "\n",
    "**Hint:** You do not need to consider the scenario where a phone number is missing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus_coords = bus_valid[(bus_valid['latitude'] != -9999) & (bus_valid['longitude'] != -9999)]\n",
    "bus_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q5a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "## üèòÔ∏è Question 5b\n",
    "\n",
    "To be close to Fisherman‚Äôs Wharf, Conan would like to find the **two restaurants** in the `bus_coords` `DataFrame` that are furthest north, breaking ties by prioritizing restaurants that are furthest east. Return the `name`, `address`, and `postal5` ZIP code of the two restaurants as a `DataFrame` and assign it to `top2`. Here is an [article](https://www.britannica.com/science/latitude) that succinctly explains latitude and longitude.\n",
    "\n",
    "**Hint:** Feel free to reference the [sort_values documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sort_values.html) to see how you can sort by multiple values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCRATCH CELL\n",
    "# Feel free to do your rough work here\n",
    "# Do not add a cell between your solution and the grader cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top2 = bus_coords.sort_values(['latitude', 'longitude'], ascending=False)[:2]\n",
    "top2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader.check(\"q5b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border: 5px solid #003262;\" />\n",
    "<hr style=\"border: 1px solid #fdb515;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bart says congratulations! You have finished Homework 2A! ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"bart.jpeg\" width = \"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Course Content Feedback\n",
    "\n",
    "If you have any feedback about this assignment or about any of our other weekly, weekly assignments, lectures, or discussions, please fill out the [Course Content Feedback Form](https://docs.google.com/forms/d/e/1FAIpQLSdA3LRTeJo_yrnecYTxq2NcwCO2iMxOgTnMAhCIO3tfQshQOw/viewform?usp=dialog). Your input is valuable in helping us improve the quality and relevance of our content to better meet your needs and expectations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission Instructions\n",
    "\n",
    "Below, you will see a cell. Running this cell will automatically generate a zip file with your autograded answers. Once you submit this file to the HW 2A Coding assignment on Pensieve, Pensieve will automatically submit a PDF file with your written answers to the HW 2A Written assignment. \n",
    "\n",
    "**Important**: Please check that your written responses were generated and submitted correctly to the HW 2A Written Assignment. \n",
    "\n",
    "**You are responsible for ensuring your submission follows our requirements and that the PDF for HW 2A written answers was generated/submitted correctly. We will not be granting regrade requests nor extensions to submissions that don't follow instructions.** If you encounter any difficulties with submission, please don't hesitate to reach out to staff prior to the deadline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(run_tests=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
