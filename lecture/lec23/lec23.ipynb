{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Lecture 23 â€“ Data 100, Summer 2025\n",
    "\n",
    "Data 100, Summer 2025\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/su25/acks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.templates[\"plotly\"].layout.colorway = px.colors.qualitative.Vivid\n",
    "px.defaults.width = 800\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import sklearn.linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll continue with the `games` dataset from last lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basketball = pd.read_csv(\"data/nba.csv\")\n",
    "\n",
    "first_team = basketball.groupby(\"GAME_ID\").first()\n",
    "second_team = basketball.groupby(\"GAME_ID\").last()\n",
    "games = first_team.merge(second_team, left_index = True, right_index = True, suffixes = [\"\", \"_OPP\"])\n",
    "\n",
    "games['GOAL_DIFF'] = games[\"FG_PCT\"] - games[\"FG_PCT_OPP\"]\n",
    "games['WON'] = (games['WL'] == \"W\").astype(int)\n",
    "games = games[['TEAM_NAME', 'TEAM_NAME_OPP', 'MATCHUP', 'WON', 'WL', 'AST', 'GOAL_DIFF']]\n",
    "\n",
    "games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we will use the `\"GOAL_DIFF\"` feature to classify whether a team won (1) or lost (0) their game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(games, x=\"GOAL_DIFF\", y=\"WON\", color=\"WL\", opacity=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Boundaries\n",
    "\n",
    "**Note: We won't go over this section together during the lecture, since the slides cover the same material.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `LogisticRegression` class of `sklearn.linear_model` behaves very similarly to the `LinearRegression` class. As before, we:\n",
    "\n",
    "1. Initialize a model object, and\n",
    "2. Fit it to our data.\n",
    "\n",
    "You find it helpful to recall the model formulation of a fitted logistic regression model with one input:\n",
    "\n",
    "$$\n",
    "\\hat{P}_{\\hat{\\theta}}(Y=1 \\mid X) = \\sigma \\left( \\hat{\\theta}_0 + \\hat{\\theta}_1 X \\right) = \\frac{1}{1 + e^{-(\\hat{\\theta}_0 + \\hat{\\theta}_1 X)}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = games[[\"GOAL_DIFF\"]]\n",
    "Y = games[\"WON\"]\n",
    "\n",
    "model = lm.LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(\"Slope:\", model.coef_[0][0])\n",
    "print(\"Intercept:\", model.intercept_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, rather than predict a numeric output, we predict the *probability* of a datapoint belonging to Class 1. We do this using the `.predict_proba` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first 10 rows\n",
    "model.predict_proba(X)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `.predict_proba` returns a 2D array. \n",
    "\n",
    "One column contains the predicted probability that the datapoint belongs to Class 0, and the other contains the predicted probability that it belongs to Class 1 (notice that all rows sum to a total probability of 1). \n",
    "\n",
    "To check which is which, we can use the `.classes_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the first column contains the probabilities of belonging to Class 0 (losing the game), and the second column contains the probabilities of belonging to Class 1 (winning). Let's grab just the probabilities of Class 1.\n",
    "\n",
    "We then apply a decision rule: Predict Class 1 if the predicted probability of belonging to Class 1 is 0.5 or higher. Otherwise, predict Class 0.\n",
    "\n",
    "- Remember that 0.5 is a common threshold, but we are not required to always use 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain P(Y=1|x) from the output.\n",
    "p = model.predict_proba(X)[:, 1]\n",
    "\n",
    "# Apply decision rule: predict Class 1 if P(Y=1|x) >= 0.5.\n",
    "(p >= 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.predict` method of `LogisticRegression` will apply a 0.5 threshold to classify data, by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .predict will automatically apply a 0.5 threshold for a logistic regression model.\n",
    "classes = model.predict(X)\n",
    "\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The point where the sigmoid function outputs 0.5 is the **decision boundary**. \n",
    "\n",
    "- This is the point where the model is indifferent between predicting Class 0 and Class 1.  \n",
    "\n",
    "- This is also the point where $\\theta_0 + \\theta_1 x = 0$. \n",
    "\n",
    "For this one dimensional case we can solve for the $x$ value of the decision boundary:\n",
    "\n",
    "$$\n",
    "x = - \\frac{\\theta_0}{\\theta_1} = - \\frac{\\text{intercept}}{\\text{slope}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " -model.intercept_[0]/model.coef_[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games[\"Predicted Class\"] = pd.Categorical(classes)\n",
    "\n",
    "test_points = pd.DataFrame({\"GOAL_DIFF\": np.linspace(-0.3, 0.3, 100)})\n",
    "test_points[\"Predicted Prob\"] = model.predict_proba(test_points)[:, 1]\n",
    "\n",
    "fig = px.scatter(games, x=\"GOAL_DIFF\", y=\"WON\", color=\"Predicted Class\", opacity=0.1)\n",
    "\n",
    "# Add the logistic regression model predictions\n",
    "fig.add_trace(go.Scatter(x=test_points[\"GOAL_DIFF\"], y=test_points[\"Predicted Prob\"], \n",
    "                         mode=\"lines\", name=\"Logistic Regression Model\", \n",
    "                         line_color=\"black\", line_width=5, line_dash=\"dash\"))\n",
    "fig.add_vline(x = -model.intercept_[0]/model.coef_[0][0], line_dash=\"dash\", \n",
    "              line_color=\"black\",\n",
    "              annotation_text=\"Decision Boundary\", \n",
    "              annotation_position=\"right\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any time the predicted probability $p$ is less than 0.5, the model predicts Class 0. Otherwise, it predicts Class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A decision boundary describes the **line** that splits the data into classes based on the *features*.\n",
    "\n",
    "For a model with one feature, the decision boundary is a *point* that separates the two classes. The number of dimensions of the decision boundary plot is the number of features.\n",
    "\n",
    "- We visualize this using a 1D plot to plot all data points in terms of *just* the feature.\n",
    "\n",
    "- We cannot define a decision boundary in terms of the predictions, so we remove that axis from our plot. \n",
    "\n",
    "Notice that all data points to the right of our decision boundary are classified as Class 1, while all data points to the left are classified as Class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(games, x=\"GOAL_DIFF\", y=np.zeros(len(games)),\n",
    "                 symbol=\"WL\", symbol_sequence=[ \"circle-open\", \"cross\"], \n",
    "                 color=\"Predicted Class\", height=300, opacity=0.1)\n",
    "# fig.update_traces(marker_symbol='line-ns-open')\n",
    "fig.update_traces(marker_size=8)\n",
    "fig.update_layout(\n",
    "    yaxis=dict(showticklabels=False, showgrid=False, zeroline=False, title=\"\"),\n",
    ")\n",
    "\n",
    "decision_boundary =  -model.intercept_[0]/model.coef_[0][0]\n",
    "fig.add_vline(x = decision_boundary, line_dash=\"dash\", \n",
    "              line_color=\"black\",\n",
    "              annotation_text=\"Decision Boundary\", \n",
    "              annotation_position=\"top right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Features\n",
    "\n",
    "**Note: We won't go over this section together during the lecture, since the slides cover the same material.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can repeat this process with a model with two features: `\"AST\"` and `\"GOAL_DIFF\"`. Now, we express a decision boundary in terms of both of these two features.\n",
    "\n",
    "How do we find the decision boundary in this case? We calculate the equation for the line that gives us all the points for which the model output is equal to the threshold.\n",
    "\n",
    "- Recall that the linear combination $x^T \\hat{\\theta}$ of logistic regression is the estimated **log odds** that a data point with features $x^T$ is Class 1.\n",
    "\n",
    "- So, we can equivalently express our decision boundary as a probability threshold or log odds threshold. Log odds are linear in $\\theta$, so our decision boundary is linear!\n",
    "\n",
    "$$\\text{Probability threshold} = T = \\frac{1}{1+e^{-\\theta_0 -\\theta_1\\times\\text{GOAL\\_DIFF}-\\theta_2\\times\\text{AST}}}$$ \n",
    "\n",
    "$$\\Longrightarrow$$\n",
    "\n",
    "$$\\text{Log odds threshold} = \\log \\frac{T}{1-T} = \\theta_0 + \\theta_1\\times\\text{GOAL\\_DIFF} + \\theta_2\\times\\text{AST}$$\n",
    "\n",
    "A probability threshold of 0.5 corresponds to a log odds threshold of $\\log \\frac{0.5}{1-0.5} = \\log (1) = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_two_feature = games[[\"GOAL_DIFF\", \"AST\"]]\n",
    "Y = games[\"WON\"]\n",
    "\n",
    "two_feature_model = lm.LogisticRegression()\n",
    "two_feature_model.fit(X_two_feature, Y)\n",
    "\n",
    "# This function plots the decision boundary such that AST is a function of GOAL_DIFF.\n",
    "theta0 = two_feature_model.intercept_\n",
    "theta1, theta2 = two_feature_model.coef_[0]\n",
    "print(theta0, theta1, theta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions using the new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games[\"Predicted Class\"] = two_feature_model.predict(X_two_feature)\n",
    "games.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following, we compute the decision boundary for this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the decision boundary\n",
    "decision_boundary = pd.DataFrame({\"GOAL_DIFF\": np.linspace(-0.3, 0.3, 100)})\n",
    "\n",
    "# Compute the y-values of the decision boundary (AST) using a grid of x-values (GOAL_DIFF).\n",
    "# The decision boundary is defined by the equation:\n",
    "# 0 = theta0 + theta1 * GOAL_DIFF + theta2 * AST\n",
    "decision_boundary[\"AST\"] = (theta0 + theta1*decision_boundary[\"GOAL_DIFF\"])/(-theta2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the decision boundary alongside the true class labels, and the predicted class labels.\n",
    "\n",
    "Notice that there are a lot of misclassifications!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "games['Predicted Class'] = pd.Categorical(games['Predicted Class'])\n",
    "fig = px.scatter(games, x=\"GOAL_DIFF\", y=\"AST\", symbol=\"WL\", \n",
    "                 hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'],\n",
    "                 color=\"Predicted Class\", \n",
    "                 symbol_sequence=[ \"circle-open\", \"cross\"],\n",
    "                 opacity=0.7,\n",
    "                 height=600)\n",
    "fig.update_traces(marker=dict(size=8))\n",
    "fig.update_layout(xaxis_range=[-0.3, 0.3], yaxis_range=[5, 50])\n",
    "# Add the decision boundary to the plot\n",
    "fig.add_scatter(x=decision_boundary[\"GOAL_DIFF\"], y=decision_boundary[\"AST\"],\n",
    "                mode=\"lines\", line_color=\"black\", line_dash=\"dash\", \n",
    "                name=\"Decision Boundary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the probabilities to the plot, with lighter shades corresponding to estimated probabilities closer to 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_diff, ast = np.meshgrid(np.linspace(-0.3, 0.3, 50), np.linspace(5, 50, 50))\n",
    "pred_grid = pd.DataFrame({\"GOAL_DIFF\": np.ravel(goal_diff), \"AST\": np.ravel(ast)})\n",
    "pred_grid['Probability'] = two_feature_model.predict_proba(pred_grid)[:, 1]\n",
    "# fig = go.Figure()\n",
    "fig.add_contour(x=pred_grid['GOAL_DIFF'], y=pred_grid['AST'], z=pred_grid['Probability'],\n",
    "                showscale=False, opacity=0.4, colorscale=\"Matter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Separability\n",
    "\n",
    "**Note: We won't go over this section together during the lecture, since the slides cover the same material.**\n",
    "\n",
    "A **linearly separable** dataset is one that can be perfectly separated into two classes by a hyperplane among the input features. \n",
    "\n",
    "- A hyperplane is a decision boundary extended to arbitrarily many dimensions. For example, a model with three features would have a 3D surface as its decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "iris = sns.load_dataset(\"iris\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is linearly separable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(iris[iris[\"species\"] != \"virginica\"], \n",
    "                 x = \"petal_length\",\n",
    "                 y = \"petal_width\", \n",
    "                 color=\"species\", \n",
    "                 symbol=\"species\", symbol_sequence=[ \"circle\", \"cross\"],\n",
    "                 render_mode=\"svg\")\n",
    "fig.update_traces(marker=dict(size=12))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this dataset is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(iris[iris[\"species\"] != \"setosa\"], \n",
    "                 x = \"petal_length\",\n",
    "                 y = \"petal_width\", \n",
    "                 color=\"species\", \n",
    "                 symbol=\"species\", symbol_sequence=[ \"circle\", \"cross\"],\n",
    "                 render_mode=\"svg\")\n",
    "fig.update_traces(marker=dict(size=12))\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When our data is linearly separable, we run the risk of **diverging weights** as the model attempts to reduce cross-entropy loss to 0.\n",
    "\n",
    "To see why, consider the following artificially generated \"toy\" dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = pd.DataFrame({\"x\": [-1, 1], \"y\": [0, 1], \"label\": pd.Categorical([0, 1])})\n",
    "fig = px.scatter(toy_df, x=\"x\", y=\"y\", \n",
    "                 color=\"label\", symbol=\"label\", \n",
    "                 symbol_sequence=[ \"circle\", \"cross\"],\n",
    "                 render_mode=\"svg\")\n",
    "fig.update_traces(marker=dict(size=12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the mean cross-entropy loss surface for this toy dataset, and a single feature model $\\hat{y} = \\sigma(\\theta x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this situation, our logistic regression model takes the form:\n",
    "\n",
    "$$ \\Large \\hat{P}_{\\theta}(Y = 1 | x) = \\sigma(\\theta_1 x) = \\frac{1}{1 + e^{-\\theta_1 x}} $$\n",
    "\n",
    "With mean cross-entropy loss:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta} &= \\underset{\\theta}{\\operatorname{argmin}}  - \\frac{1}{n} \\sum_{i=1}^n \\left( y_i \\log (\\sigma(\\theta_1 x_i) + (1 - y_i) \\log (1 - \\sigma(\\theta_1 x_i)) \\right) \\\\\n",
    "&= \\underset{\\theta}{\\operatorname{argmin}} -\\frac{1}{2} \\left[ \\log (\\sigma( -  \\theta_1 )) + \\log(1 - \\sigma(\\theta_1))\\right]\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_model(theta1, x):\n",
    "    return 1/(1 + np.exp(-theta1 * x))\n",
    "\n",
    "def mean_cross_entropy_loss_toy(theta1):\n",
    "    # Here we use 1 - sigma(z) = sigma(-z) to improve numerical stability.\n",
    "    return - np.sum(toy_df['y'] * np.log(toy_model(theta1, toy_df['x'])) + \\\n",
    "                    (1-toy_df['y']) * np.log(toy_model(theta1, -toy_df['x'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetas = np.linspace(-30, 30, 100)\n",
    "fig = px.line(x=thetas, y = [mean_cross_entropy_loss_toy(theta) for theta in thetas], \n",
    "              render_mode=\"svg\")\n",
    "fig.update_layout(xaxis_title=\"Theta\", yaxis_title=\"Mean CE Loss\",\n",
    "                  title=\"Mean Cross Entropy Loss for Toy Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch the y-axis to log scale to better visualize the loss surface for larger $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(x=thetas, y = [mean_cross_entropy_loss_toy(theta) for theta in thetas],\n",
    "              log_y=True, render_mode=\"svg\")\n",
    "fig.update_layout(xaxis_title=\"Theta\", yaxis_title=\"Log Scale Mean CE Loss\",\n",
    "                  title=\"Log Scale Mean Cross Entropy Loss for Toy Example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can keep decreasing the loss if we increase the value of $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If left unchecked, the logistic regression model will attempt to use *infinite* values as the \"optimal\" model parameters. We describe this phenomenon as the model weights \"**diverging**\". \n",
    "\n",
    "We can use **regularization** to restrict how large the model parameters can be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_loss_toy(theta1, reg):\n",
    "    return mean_cross_entropy_loss_toy(theta1) + reg * theta1**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = 0.01 # Small amount of regularization\n",
    "fig = px.line(x=thetas, y = [regularized_loss_toy(theta, reg) for theta in thetas],\n",
    "              render_mode=\"svg\")\n",
    "fig.update_layout(xaxis_title=\"Theta\", yaxis_title=\"Mean CE Loss\",\n",
    "                  title=f\"Mean Cross Entropy Loss for Toy Example (Regularization = {reg})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ã€°ï¸ Regularization in `sklearn`\n",
    "\n",
    "**This is the only section we will go over together during lecture!**\n",
    "\n",
    "By default, `sklearn`'s `LogisticRegression` applies an arbitrary amount of regularization for us. \n",
    "\n",
    "Here, a **larger** C implies **less** regularization. $\\lambda=\\frac{1}{C}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression objects behave a lot like LinearRegression objects.\n",
    "# L2 regularization is applied by default, where lambda = 1 / C.\n",
    "# Bigger C means less regularization.\n",
    "toy_model = lm.LogisticRegression(C=10)\n",
    "\n",
    "# We fit to two data points: (-1, 0) and (1, 1).\n",
    "toy_model.fit([[-1], [1]], [0,1])\n",
    "\n",
    "# Generate estimated probabilities across a range of x-values.\n",
    "xtest = np.linspace(-1.5, 1.5, 1000)[:, np.newaxis]\n",
    "p = toy_model.predict_proba(xtest)[:,1]\n",
    "\n",
    "fig = px.scatter(toy_df, x=\"x\", y=\"y\", \n",
    "         color=\"label\", symbol=\"label\", \n",
    "         symbol_sequence=[\"circle\", \"cross\"],\n",
    "         title=f\"LR Fit (slope = {toy_model.coef_[0][0]}, intercept = {toy_model.intercept_[0]})\",\n",
    "         render_mode=\"svg\")\n",
    "fig.update_traces(marker=dict(size=15))\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.add_scatter(x=np.ravel(xtest), y=p, mode=\"lines\", name=\"LR Model with C=10\", \n",
    "                line_color=\"black\", opacity=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we reduce the amount of regularization, notice the slope term is larger, so the curve fits the two data points more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit exactly the same model, but reduce the regularization strength by \n",
    "# a factor of 100.\n",
    "toy_model = lm.LogisticRegression(C=1000)\n",
    "toy_model.fit([[-1], [1]], [0,1])\n",
    "\n",
    "xtest = np.linspace(-1.5, 1.5, 1000)[:, np.newaxis]\n",
    "p = toy_model.predict_proba(xtest)[:,1]\n",
    "\n",
    "fig = px.scatter(toy_df, x=\"x\", y=\"y\", \n",
    "                 color=\"label\", symbol=\"label\", \n",
    "                 symbol_sequence=[ \"circle\", \"cross\"],\n",
    "                 title=f\"LR Fit (slope = {toy_model.coef_[0][0]}, intercept = {toy_model.intercept_[0]})\",\n",
    "                 render_mode=\"svg\")\n",
    "fig.update_traces(marker=dict(size=15))\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.add_scatter(x=np.ravel(xtest), y=p, mode=\"lines\", name=\"LR model with C=1000\", \n",
    "                line_color=\"black\", opacity=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Performance Metrics\n",
    "\n",
    "**Note: We won't cover this section together during lecture, since the same material is covered in the slides.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our `games` data. We'll compute the **accuracy** of our `model` on this data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, Y):\n",
    "    return np.mean(model.predict(X) == Y)\n",
    "\n",
    "print(model.predict(X)[:5])\n",
    "print(Y[:5])\n",
    "accuracy(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per usual, `scikit-learn` can do this for us. The `.score` method of a `LogisticRegression` classifier provides the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important Note: `model.predict` and `model.score` use a threshold of 0.5.\n",
    "To use a different threshold, you must use `model.predict_proba` and work with probabilities directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "`scikit-learn` has an built-in `confusion_matrix` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Be careful â€“ confusion_matrix takes in y_true as the first parameter and y_pred as the second.\n",
    "# Don't mix these up!\n",
    "cm = confusion_matrix(Y, model.predict(X))\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.imshow(cm, x=[\"0\", \"1\"], y=[\"0\", \"1\"],\n",
    "          labels=dict(x=\"Predicted\", y=\"Actual\"), \n",
    "          text_auto=True, \n",
    "          color_continuous_scale=\"Blues\", \n",
    "          width=400, height=400)\n",
    "fig.update_xaxes(side=\"top\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "We can also compute the number of TP, TN, FP, and TN for our classifier, and then its precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = model.predict(X)\n",
    "tp = np.sum((Y_hat == 1) & (Y == 1))\n",
    "tn = np.sum((Y_hat == 0) & (Y == 0))\n",
    "\n",
    "fp = np.sum((Y_hat == 1) & (Y == 0))\n",
    "fn = np.sum((Y_hat == 0) & (Y == 1))\n",
    "\n",
    "\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"True Negatives: \", tn)\n",
    "print(\"False Positives:\", fp)\n",
    "print(\"False Negatives:\", fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These numbers match what we see in the confusion matrix above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall\n",
    "\n",
    "**Precision** -- How precise are my positive predictions? In other words, what fraction of the things the model predicted positive are actually positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** -- What proportion of actual positives did my model recall in its predictions? In other words, what proportion of actual positive cases that were correctly identified by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True and False Positive Rates\n",
    "\n",
    "The TP, TN, FP, and TN we just calculated also allow us to compute the true and false positive rates (TPR and FPR). Recall that TPR is the same as recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr = fp/(fp + tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp/(tp + fn)\n",
    "tpr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to remember that these values are all for the threshold of $T = 0.5$, which is `scikit-learn`'s default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ›ï¸ Adjusting the Classification Threshold\n",
    "\n",
    "**Note: We won't go through this section of the demo together during lecture, since the slides cover the same material.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, we used a threshold of 0.5 in our decision rule: If the predicted probability was greater than 0.5 we predicted Class 1, otherwise, we predicted Class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = games[[\"GOAL_DIFF\"]]\n",
    "Y = games[\"WON\"]\n",
    "model = lm.LogisticRegression()\n",
    "model.fit(X, Y)\n",
    "print(\"Slope:\", model.coef_[0][0])\n",
    "print(\"Intercept:\", model.intercept_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(threshold = 0.5):\n",
    "    games[\"Predicted Class\"] = model.predict_proba(X)[:, 1] >= threshold\n",
    "    # Needed for plotting\n",
    "    games[\"Predicted Class\"] = pd.Categorical(games[\"Predicted Class\"])\n",
    "    fig = px.scatter(games, \n",
    "            x=\"GOAL_DIFF\", y=\"WON\", color=\"Predicted Class\", \n",
    "            title=f\"Logistic Regression Predictions (Threshold = {threshold})\")\n",
    "    # Add the logistic regression model predictions\n",
    "    # Make the data points for the LR model curve\n",
    "    test_points = pd.DataFrame({\"GOAL_DIFF\": np.linspace(-0.3, 0.3, 100)})\n",
    "    test_points[\"Predicted Prob\"] = model.predict_proba(test_points)[:, 1]\n",
    "    fig.add_trace(go.Scatter(x=test_points[\"GOAL_DIFF\"], y=test_points[\"Predicted Prob\"], \n",
    "                            mode=\"lines\", name=\"Logistic Regression Model\", \n",
    "                            line_color=\"black\", line_width=5, line_dash=\"dash\"))\n",
    "    decision_boundary = (-np.log(1/threshold - 1) - model.intercept_[0])/model.coef_[0][0]\n",
    "    fig.add_vline(x = decision_boundary, line_dash=\"dash\", line_color=\"black\",\n",
    "                  annotation_text=\"Decision Boundary\", annotation_position=\"right\")\n",
    "    return fig\n",
    "\n",
    "plot_predictions(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we change the threshold? Below, we apply a threshold of $T=0.25$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we **lower the threshold**, we require a lower predicted probability before we predict Class 1. We can think of this as us telling our model that it needs to be less \"confident\" about a data point being Class 1 before making a positive prediction. The total number of data points predicted to be Class 1 **either stays the same or increases**.\n",
    "\n",
    "The converse happens if we raise the threshold. Consider setting $T=0.75$. Now, we require a higher predicted probability before we predict Class 1. The total number of data points predicted to be Class 1 decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thresholds and Performance Metrics\n",
    "\n",
    "**Note: We won't cover this section together during lecture, since the same material is covered in the slides.**\n",
    "\n",
    "How does changing the threshold impact our performance metrics?\n",
    "\n",
    "Let's run an experiment: we'll test out several different possible thresholds. \n",
    "\n",
    "- For each threshold $T$, we'll make a decision rule where we classify any point with a predicted probability equal to or greater than $T$ as being in Class 1. \n",
    "\n",
    "- Otherwise, we'll predict Class 0. \n",
    "\n",
    "- We'll then compute the overall accuracy of the classifier when using that threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define performance metrics dependent on the threshold value.\n",
    "def predict_threshold(model, X, T): \n",
    "    prob_one = model.predict_proba(X)[:, 1]\n",
    "    return (prob_one >= T).astype(int)\n",
    "\n",
    "def accuracy_threshold(X, Y, T):\n",
    "    return np.mean(predict_threshold(model, X, T) == Y)\n",
    "\n",
    "def precision_threshold(X, Y, T):\n",
    "    Y_hat = predict_threshold(model, X, T)\n",
    "    denominator = np.sum(Y_hat == 1)\n",
    "    if denominator == 0:\n",
    "        denominator = np.nan\n",
    "    return np.sum((Y_hat == 1) & (Y == 1)) / denominator\n",
    "    \n",
    "def recall_threshold(X, Y, T):\n",
    "    Y_hat = predict_threshold(model, X, T)\n",
    "    return np.sum((Y_hat == 1) & (Y == 1)) / np.sum(Y == 1)\n",
    "\n",
    "def tpr_threshold(X, Y, T): # Same as recall\n",
    "    Y_hat = predict_threshold(model, X, T)\n",
    "    return np.sum((Y_hat == 1) & (Y == 1)) / np.sum(Y == 1)\n",
    "\n",
    "def fpr_threshold(X, Y, T):\n",
    "    Y_hat = predict_threshold(model, X, T)\n",
    "    return np.sum((Y_hat == 1) & (Y == 0)) / np.sum(Y == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame()\n",
    "metrics[\"Threshold\"] = np.linspace(0, 1, 1000)\n",
    "metrics[\"Accuracy\"] = [accuracy_threshold(X, Y, t) for t in metrics[\"Threshold\"]]\n",
    "metrics[\"Precision\"] = [precision_threshold(X, Y, t) for t in metrics[\"Threshold\"]]\n",
    "metrics[\"Recall\"] = [recall_threshold(X, Y, t) for t in metrics[\"Threshold\"]]\n",
    "metrics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics, \n",
    "        x=\"Threshold\", y=\"Accuracy\",\n",
    "        title=\"Accuracy vs. Threshold\",\n",
    "        render_mode=\"svg\", width=600, height=600)\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the threshold that maximizes accuracy we find it is close $T=0.49$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The threshold that maximizes accuracy.\n",
    "metrics.sort_values(\"Accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that setting $T=0.5$ does not always result in the best performance! Part of the model design process for classification includes **choosing an appropriate threshold value**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall Curves\n",
    "In the lecture, we noted that there is a tradeoff between precision and recall.\n",
    "\n",
    "Precision $=\\frac{TP}{\\text{Positive Predictions}}=\\frac{TP}{TP+FP}$ increases as the number of false positives decreases, which occurs as the threshold is raised, since raising the threshold tends to reduce the number of positive predictions.\n",
    "\n",
    "Recall $=\\frac{TP}{\\text{Actual Class 1s}}=\\frac{TP}{TP+FN}$ increases as the number of false negatives decreases, which occurs as the threshold is lowered, since lowering the threshold tends to decrease number of negative predictions.\n",
    "\n",
    "We want to keep both precision and recall high. To do so, we'll need to strategically choose a threshold value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics, \n",
    "        x=\"Threshold\", y=[\"Accuracy\", \"Precision\", \"Recall\"],\n",
    "        title=\"Performance Metrics vs. Threshold\",\n",
    "        render_mode=\"svg\", height=600, width=600)\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **precision-recall** curve tests out many possible thresholds. Each point on the curve represents the precision and recall of the classifier for a *particular choice of threshold*.\n",
    "\n",
    "We choose a threshold value that keeps both precision and recall high (usually in the rightmost \"corner\" of the curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics, x=\"Recall\", y=\"Precision\",\n",
    "        title=\"Precision vs. Recall\",\n",
    "        width=600, height=600,\n",
    "        render_mode=\"svg\")\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to balance precision and recall is to compute the **F1 score**. The F1 score is the harmonic mean of precision and recall:\n",
    "\n",
    "$$F1 = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"F1\"] = (2 * metrics[\"Precision\"] * metrics[\"Recall\"] \n",
    "                     / (metrics[\"Precision\"] + metrics[\"Recall\"]))\n",
    "ind = metrics['F1'].idxmax()\n",
    "metrics.loc[ind,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = px.line(metrics, x=\"Threshold\", y=\"F1\",\n",
    "              title=\"Finding F1 Score Maximum\",\n",
    "              render_mode=\"svg\",\n",
    "              height=600, width=600)\n",
    "\n",
    "fig.add_scatter(x=[metrics.loc[ind, 'Threshold']], y=[metrics.loc[ind, 'F1']], \n",
    "                mode='markers', marker=dict(size=10, color='red'),\n",
    "                name=f\"F1 Max {metrics.loc[ind, 'Threshold']:.5f}\",)\n",
    "\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics, x=\"Recall\", y=\"Precision\",\n",
    "              title=\"Precision vs. Recall\", width=600, height=600,\n",
    "              render_mode=\"svg\")\n",
    "fig.add_scatter(x=[metrics.loc[ind, 'Recall']], y=[metrics.loc[ind, 'Precision']], \n",
    "                mode='markers', marker=dict(size=10, color='red'),\n",
    "                name=f\"F1 Max {metrics.loc[ind, 'Threshold']:.5f}\")\n",
    "fig.update_layout(legend=dict(x=.5, y=.1))\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves\n",
    "\n",
    "We can repeat a similar experiment for the FPR and TPR. Remember that we want to keep FPR *low* and TPR *high*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"TPR\"] = [tpr_threshold(X, Y, t) for t in metrics[\"Threshold\"]]\n",
    "metrics[\"FPR\"] = [fpr_threshold(X, Y, t) for t in metrics[\"Threshold\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics, x=\"Threshold\", y=[\"TPR\", \"FPR\", \"Accuracy\"],\n",
    "        render_mode=\"svg\", width=600, height=600)\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **ROC curve** tests many possible decision rule thresholds. For each possible threshold, it plots the corresponding TPR and FPR of the classifier.\n",
    "\n",
    "\"ROC\" stands for \"Receiver Operating Characteristic\". It comes from the field of signal processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics, x=\"FPR\", y=\"TPR\", title=\"ROC Curve\", \n",
    "        width=600, height=600,\n",
    "        render_mode=\"svg\")\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Ideally, a perfect classifier would have a FPR of 0 and TPR of 1. The area under the perfect classifier is 1. \n",
    " \n",
    " We often use the area under the ROC curve (abbreviated \"AUC\") as an indicator of model performance. The closer the AUC is to 1, the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(metrics, x=\"FPR\", y=\"TPR\", title=\"ROC Curve\", \n",
    "              width=600, height=600,\n",
    "              render_mode=\"svg\")\n",
    "fig.add_scatter(x=[0,0,1], y=[0,1,1], mode='lines', \n",
    "                line_dash='dash', line_color='black',\n",
    "                name=\"Perfect Classifier\")\n",
    "# move the legend inside the plot\n",
    "fig.update_layout(legend=dict(x=.5, y=.1))\n",
    "fig.update_layout(\n",
    "  xaxis_title=dict(font=dict(size=22)),\n",
    "  yaxis_title=dict(font=dict(size=22))\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
