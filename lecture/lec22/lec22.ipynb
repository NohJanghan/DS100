{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚õπÔ∏è Lecture 22, Logistic Regression I ‚Äì Data 100, Summer 2025\n",
    "\n",
    "Data 100, Summer 2025\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/su25/acks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "pio.templates[\"plotly\"].layout.colorway = px.colors.qualitative.Vivid\n",
    "px.defaults.width = 800\n",
    "\n",
    "# Set default plotly layout\n",
    "pio.templates[\"plotly\"].layout.font.size = 22\n",
    "\n",
    "\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we will work with the `basketball` dataset, which contains information about basketball games played in the NBA. Our goal is to predict whether or not a team wins their game (`\"WON\"`) given their `\"GOAL_DIFF\"`. \n",
    "\n",
    "- The variable `\"GOAL_DIFF\"` represents the difference in successful field goal rates between the two teams competing in a game. Field goals are any shot other than a free throw. \n",
    "\n",
    "- A positive value for `\"GOAL_DIFF\"` means that a team had more successful field goal attempts, on average, than their opponent. \n",
    "\n",
    "- Keep in mind, a team could have low field goal success rate and still win the game, so long as they had more field goal attempts than their opponent. \n",
    "\n",
    "In the cell below, we perform data cleaning to transform the data into a useful form, which we store as the DataFrame `basketball`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basketball = pd.read_csv(\"data/nba.csv\")\n",
    "basketball.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basketball = pd.read_csv(\"data/nba.csv\")\n",
    "\n",
    "# Extract the team names from each game-team, and organize the data so that \n",
    "# each row corresponds to a game, not a game-team combo.\n",
    "first_team = basketball.groupby(\"GAME_ID\").first()\n",
    "second_team = basketball.groupby(\"GAME_ID\").last()\n",
    "games = first_team.merge(second_team, left_index = True, right_index = True, suffixes = [\"\", \"_OPP\"])\n",
    "\n",
    "# Compute the field goal success rate\n",
    "games['GOAL_DIFF'] = games[\"FG_PCT\"] - games[\"FG_PCT_OPP\"]\n",
    "\n",
    "games['WON'] = (games['WL'] == \"W\").astype(int)\n",
    "games = games[['TEAM_NAME', 'TEAM_NAME_OPP', 'MATCHUP', 'WON', 'WL', 'GOAL_DIFF']]\n",
    "\n",
    "games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü™ô Logistic Regression\n",
    "\n",
    "**Note: We won't work through this section together in the lecture, since the slides provide the same information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we visualize our data, we see a very different pattern than than the scatter plots from simple linear regression. \n",
    "\n",
    "Because a team can only win or lose a game, the only possible values of `\"WON\"` are 1 (if the team won the game) or 0 (if the team lost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(games, \n",
    "           x=\"GOAL_DIFF\", y=\"WON\", color=\"WL\", \n",
    "           hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the only possible values of `\"WON\"` are 0 or 1, the visualization above shows significant overplotting. \n",
    "\n",
    "We can modify the transparency of the points to better see the data density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the transparency of the points\n",
    "px.scatter(games, \n",
    "           x=\"GOAL_DIFF\", y=\"WON\", color=\"WL\", \n",
    "           hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'],\n",
    "           opacity=0.1\n",
    "           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary least squares (OLS) regression is intended to output **continuous** numeric predictions.\n",
    "\n",
    "Given our binary output data, OLS fits the data poorly.\n",
    "\n",
    "- However, it is common in the social sciences to fit an OLS model like this due to the nice interpretability of OLS coefficients, even though the fit is not as good as logistic regression. See [Linear Probability Models](https://en.wikipedia.org/wiki/Linear_probability_model#:~:text=In%20statistics%2C%20a%20linear%20probability,one%20or%20more%20explanatory%20variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear regression model to the data\n",
    "X = games[[\"GOAL_DIFF\"]]\n",
    "Y = games[\"WON\"]\n",
    "least_squares_model = lm.LinearRegression()\n",
    "least_squares_model.fit(X, Y)\n",
    "\n",
    "# Make some predictions for a range of GOAL_DIFF values\n",
    "pred = pd.DataFrame({\"GOAL_DIFF\": np.linspace(-0.3, 0.3)})\n",
    "pred[\"LS_Pred\"] = least_squares_model.predict(pred)\n",
    "\n",
    "# Visualize the model\n",
    "fig = px.scatter(games, \n",
    "           x=\"GOAL_DIFF\", y=\"WON\", color=\"WL\", \n",
    "           hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'], \n",
    "           opacity=0.1)\n",
    "fig.add_trace(go.Scatter(x=pred[\"GOAL_DIFF\"], y=pred[\"LS_Pred\"], \n",
    "                         mode=\"lines\", name=\"Least Squares Fit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS fits the data poorly. We will need a new approach to modeling binary outcomes.\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "**Instructor Note: Return to Lecture!**\n",
    "\n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóëÔ∏è Binning and Averaging\n",
    "\n",
    "**Note: We won't work through this section together in the lecture, since the slides provide the same information.**\n",
    "\n",
    "Back in Data 8, you built up your understanding of linear regression by first considering the [graph of averages](https://inferentialthinking.com/chapters/08/1/Applying_a_Function_to_a_Column.html#example-prediction). \n",
    "\n",
    "We construct a graph of averages by *binning* all $x$ data into bins of similar values, then computing the average value of $y$ for each bin. This graph gives us a rough indication of the relationship between $x$ and $y$.\n",
    "\n",
    "Let's try this on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Break our GOAL_DIFF values into 20 equally-spaced bins from min to max\n",
    "# `bins` contains the lower, upper values for the interval for that row\n",
    "# `cuts` contains the unique bin edges\n",
    "# retbins=True tells pd.cut to provide the outermost bin edges\n",
    "bins, cuts = pd.cut(games[\"GOAL_DIFF\"], 20, retbins=True)\n",
    "\n",
    "print('Edges of the 20 bins:')\n",
    "print(cuts)\n",
    "print()\n",
    "\n",
    "print('First 5 GOAL_DIFF values:')\n",
    "print(games[\"GOAL_DIFF\"][:5])\n",
    "print()\n",
    "\n",
    "print('First 5 observations categorized into bins:')\n",
    "print(bins[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the bins to the original data\n",
    "games.join(bins, rsuffix=\"_bins\").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(games, \n",
    "           x=\"GOAL_DIFF\", y=\"WON\", color=\"WL\", opacity=0.1,\n",
    "           hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'])\n",
    "\n",
    "for cut in cuts:\n",
    "    fig.add_vline(x=cut, line_dash=\"dash\", line_color=\"black\", opacity=0.5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each bin, we can compute the win rate within that bin.  We do this by grouping the data according to which center of bin each row is assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the bin center for every game.\n",
    "# This way, all the games that are in the same bin will have the same bin_center.\n",
    "games['bin_center'] = bins.apply(lambda x: (x.left + x.right)/2).astype(float)\n",
    "\n",
    "# We group by bin center and compute the average of the label to \n",
    "# get the win rate for each bin.\n",
    "win_rates_by_bin = (\n",
    "    games[[\"bin_center\", \"WON\"]]\n",
    "    .groupby(\"bin_center\") \n",
    "    .mean()\n",
    "    .rename(columns={\"WON\": \"Win Rate\"})\n",
    ")\n",
    "win_rates_by_bin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model\n",
    "fig = px.scatter(games, \n",
    "           x=\"GOAL_DIFF\", y=\"WON\", color=\"WL\", opacity=0.1,\n",
    "           hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'])\n",
    "fig.add_trace(go.Scatter(x=win_rates_by_bin.index, y=win_rates_by_bin['Win Rate'], \n",
    "                         mode=\"markers+lines\", name=\"Win Rate by Bin\"))\n",
    "for cut in cuts:\n",
    "    fig.add_vline(x=cut, line_dash=\"dash\", line_color=\"black\", opacity=0.1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our graph of averages has revealed a S-shaped curve. This doesn't look like anything we've encountered before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü™ó Modeling the S-shaped curve\n",
    "\n",
    "**Note: We won't work through this section together in the lecture, since the slides provide the same information.**\n",
    "\n",
    "The relationship between $x$ (`\"GOAL_DIFF\"`) and $y$ (`\"WON\"`) shows clear non-linearity. \n",
    "\n",
    "A few observations:\n",
    "* All predictions on our curve are between 0 and 1.\n",
    "* To compute the average for each bin, we calculated: \n",
    "\n",
    "$$\\frac{\\#\\:Y=1\\:\\text{in bin}}{\\#\\:\\text{datapoints in bin}} = P(Y=1 | \\text{bin}).$$\n",
    "\n",
    "Together, these observations indicate that our graph of averages is *actually* modeling the probability of a data point having $Y = 1$! \n",
    "\n",
    "- In other words, rather than predicting a continuous numeric output on $(-\\infty, \\infty)$ akin to OLS, we now want to predict the *probability* of a datapoint belonging to Class 1 (i.e., the team winning the game) on $[0,1]$.\n",
    "\n",
    "So, we need to identify a function that maps continuous inputs on $(-\\infty, \\infty)$ to outputs on $[0,1]$.\n",
    "\n",
    "The logarithm maps inputs on $(0, \\infty)$ to outputs on $(-\\infty, \\infty)$, so it won't work for our purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the logarithm function\n",
    "x = np.linspace(-1, 3, 100)\n",
    "y = np.log(x)\n",
    "fig = px.line(x=x, y=y)\n",
    "fig.add_hline(y=0, line_color=\"black\", line_dash=\"dash\", opacity=0.5)\n",
    "fig.add_vline(x=0, line_color=\"black\", line_dash=\"dash\", opacity=0.5)\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"log(x)\",\n",
    "    title=\"Logarithm Function\",\n",
    "    width=800,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exponential function maps inputs on $(-\\infty, \\infty)$ to outputs on $(0, \\infty)$, so it also won't work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the logarithm function\n",
    "x = np.linspace(-1, 3, 100)\n",
    "y = np.exp(x)\n",
    "fig = px.line(x=x, y=y)\n",
    "fig.add_hline(y=0, line_color=\"black\", line_dash=\"dash\", opacity=0.5)\n",
    "fig.add_vline(x=0, line_color=\"black\", line_dash=\"dash\", opacity=0.5)\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"log(x)\",\n",
    "    title=\"Exponential Function\",\n",
    "    width=800,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **sigmoid ($\\sigma$) function** is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1+e^{-x}} = \\frac{e^x}{1+e^x}\n",
    "$$\n",
    "\n",
    "The sigmoid function maps inputs on $(-\\infty, \\infty)$ to outputs on $(0, 1)$, akin to our S-shaped curve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sigmoid function\n",
    "x = np.linspace(-10, 10, 1000)\n",
    "y = 1/(1+np.exp(-x))\n",
    "fig = px.line(x=x, y=y)\n",
    "fig.add_hline(y=0, line_color=\"black\", line_dash=\"dash\", opacity=0.5)\n",
    "fig.add_vline(x=0, line_color=\"black\", line_dash=\"dash\", opacity=0.5)\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"log(x)\",\n",
    "    title=\"Sigmoid Function\",\n",
    "    width=800,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, the **logistic regression model** fits a sigmoid function to the data. For the purposes of fitting closely to data with binary (0/1) outputs, logistic regression is superior to a straight line fit like SLR.\n",
    "\n",
    "Below, we fit a logistic regression model to the basketball data. Don't worry about what's going on \"under the hood\" for now. Just notice how similar the logistic regression model is to our graph of average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_model = lm.LogisticRegression(C=20)\n",
    "logistic_model.fit(X, Y)\n",
    "pred[\"Logistic_Pred\"] = logistic_model.predict_proba(pred[[\"GOAL_DIFF\"]])[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a logistic regression model superimposed on the data\n",
    "fig = px.scatter(games, \n",
    "           x=\"GOAL_DIFF\", y=\"WON\", color=\"WL\", opacity=0.1,\n",
    "           hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'])\n",
    "# Add the binned predictions\n",
    "fig.add_trace(go.Scatter(x=win_rates_by_bin.index, y=win_rates_by_bin['Win Rate'], \n",
    "                         mode=\"markers+lines\", name=\"Win Rate by Bin\"))\n",
    "# Add the logistic regression model predictions\n",
    "fig.add_trace(go.Scatter(x=pred[\"GOAL_DIFF\"], y=pred[\"Logistic_Pred\"], \n",
    "                         mode=\"lines\", name=\"Logistic Regression Model\", \n",
    "                         line_color=\"black\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Akin to the OLS hyperplane, we're not limited to just one input to the sigmoid function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a 3D sigmoid surface\n",
    "x1 = np.linspace(-10, 10, 100)\n",
    "x2 = np.linspace(-10, 10, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "Y = 1/(1 + np.exp(-(X1 + X2)))\n",
    "\n",
    "# Create a 3D surface plot for the sigmoid function\n",
    "fig = go.Figure(data=[go.Surface(z=Y, x=X1[0], y=X2[:, 0])])\n",
    "\n",
    "# Update layout for better visualization\n",
    "fig.update_layout(\n",
    "  title=\"Sigmoid with two inputs\",\n",
    "  scene=dict(\n",
    "    xaxis_title=\"X1\",\n",
    "    yaxis_title=\"X2\",\n",
    "    zaxis_title=\"\\u03C3(X1, X2)\"\n",
    "  ),\n",
    "  width=800,\n",
    "  height=600\n",
    ")\n",
    "\n",
    "# Reduce tick label size\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(tickfont=dict(size=14)),\n",
    "        yaxis=dict(tickfont=dict(size=14)),\n",
    "        zaxis=dict(tickfont=dict(size=14))\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified version of earlier plot used in the lecture slides\n",
    "fig = px.scatter(games, \n",
    "           x=\"GOAL_DIFF\", y=\"WON\", color=\"WL\", opacity=0.1,\n",
    "           hover_data=['TEAM_NAME', 'TEAM_NAME_OPP'],\n",
    "           # change x label\n",
    "           labels={\"GOAL_DIFF\": \"X\",\n",
    "                   \"WON\": \"P(Y=1|X)\"},\n",
    "           )\n",
    "\n",
    "# Add the logistic regression model predictions\n",
    "fig.add_trace(go.Scatter(x=pred[\"GOAL_DIFF\"], y=pred[\"Logistic_Pred\"], \n",
    "                         mode=\"lines\", name=\"Logistic Regression Model\", \n",
    "                         line_color=\"black\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lecture figure of the model with the log odds outcome\n",
    "fig = px.line(x=pred[\"GOAL_DIFF\"], \n",
    "  y=np.log(pred[\"Logistic_Pred\"] / (1-pred[\"Logistic_Pred\"])),\n",
    "  labels={\"x\": \"X\", \"y\": \"log Odds(Y=1|X)\"})\n",
    "\n",
    "fig.update_traces(line=dict(color='black'))\n",
    "\n",
    "# add horizontal line at y=0\n",
    "fig.add_hline(y=0, line_color=\"black\", line_dash=\"dash\", opacity=0.5)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öîÔ∏è Cross-Entropy Loss\n",
    "\n",
    "**Note: We won't work through this section together in the lecture, since the slides provide the same information.**\n",
    "\n",
    "To identify the optimal model parameters for our logistic regression model, we need to define a loss function. We might be inclined to use our familiar mean squared error (MSE). It turns out this is a bad idea.\n",
    "\n",
    "In the cell below, we artificially generate a \"toy\" dataset to explore the loss of a logistic regression model. We'll try to use the `\"x\"` feature to predict the `\"y\"` target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_df = pd.DataFrame({\n",
    "        \"x\": [-4, -2, -0.5, 1, 3, 5],\n",
    "        \"y\": [0, 0, 1, 0, 1, 1]\n",
    "})\n",
    "toy_df[\"str_y\"] = toy_df[\"y\"].astype(str)\n",
    "toy_df.sort_values(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(toy_df, x=\"x\", y=\"y\", color=\"str_y\", width=800)\n",
    "fig.update_traces(marker_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the loss surface for this toy data using MSE with the model $\\hat{y} = \\sigma(x\\theta)$. We don't include an intercept term, so $\\theta$ and $x$ are both scalars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sigmoid function as the example model\n",
    "def sigmoid(x_theta):\n",
    "    return 1/(1+np.e**-x_theta)\n",
    "\n",
    "# Compute the MSE of the fitted model for a given theta\n",
    "def mse_on_toy_data(theta):\n",
    "    p_hat = sigmoid(toy_df['x'] * theta)\n",
    "    return np.mean((toy_df['y'] - p_hat)**2)\n",
    "\n",
    "# Compute the MSE for a range of theta values\n",
    "theta_loss = pd.DataFrame({\"theta\": np.linspace(-10, 10, 100)})\n",
    "theta_loss[\"MSE\"] = theta_loss[\"theta\"].apply(mse_on_toy_data)\n",
    "px.line(theta_loss, x=\"theta\", y=\"MSE\", width=800,\n",
    "        title=\"MSE on Toy Classification Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss surface is not convex! In other words, there are multiple local minima in the loss surface. This means that, depending on where we start our optimization search, we'll end up with different results for the optimizing $\\theta$. Let's explore with `scipy.optimize.minimize`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the initial guess as theta = 0\n",
    "best_theta = minimize(mse_on_toy_data, x0 = 0)[\"x\"][0]\n",
    "best_theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This \"optimized\" value of $\\theta$ produces the following model when we apply it to our model $\\hat{y} = \\sigma(\\theta x )$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(toy_df, x=\"x\", y=\"y\", color=\"str_y\", width=800)\n",
    "xs = np.linspace(-10, 10, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=sigmoid(xs * best_theta), \n",
    "    mode=\"lines\", line_color=\"black\", \n",
    "    name=f\"LR Model: theta = {best_theta:.2f}\"))\n",
    "fig.update_traces(marker_size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different starting point for the initial guess for the minimizing parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the initial guess as theta = -5\n",
    "best_theta_2 = minimize(mse_on_toy_data, x0 = -5)[\"x\"][0]\n",
    "best_theta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh, looks like the optimizer got stuck at a local minimum of the loss surface. If we use this guess for the optimal $\\theta$ in our logistic regression model, we see strange behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(toy_df, x=\"x\", y=\"y\", color=\"str_y\", width=800)\n",
    "xs = np.linspace(-10, 10, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=sigmoid(xs * best_theta_2), \n",
    "    mode=\"lines\", line_color=\"black\", \n",
    "    name=f\"LR Model: theta = {best_theta_2:.2f}\"))\n",
    "fig.update_traces(marker_size=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what went wrong, let's plot these two \"optimized\" guesses for $\\hat{\\theta}$ on the original loss surface. They correspond to the local and global minimum of the loss surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(theta_loss, x=\"theta\", y=\"MSE\", width=800,\n",
    "              title=\"MSE on Toy Classification Data\")\n",
    "fig.add_scatter(x=[best_theta], y=[mse_on_toy_data(best_theta)], \n",
    "                mode=\"markers\", marker_size=10, marker_color=\"red\",\n",
    "                name=f\"Theta_1: {best_theta:.2f}\")\n",
    "fig.add_scatter(x=[best_theta_2], y=[mse_on_toy_data(best_theta_2)], \n",
    "                mode=\"markers\", marker_size=10, marker_color=\"red\",\n",
    "                name=f\"Theta_2: {best_theta_2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen now that MSE is not convex for logistic regression, which leads to difficulty in optimizing $\\hat{\\theta}$. \n",
    "\n",
    "Beyond this issue, the squared loss isn't well-suited for a probability task. Since $\\hat{p}_i$ is between 0 and 1, and $y_i$ is either 0 or 1, the squared loss for a single point $(y_i - \\hat{p}_i)^2$ is bounded between 0 and 1.\n",
    "\n",
    "What this means in practice: Even if our prediction is **terrible**, the squared loss is never that large. \n",
    "\n",
    "- Consider the \"worst-case scenario\" where the true class $y_i$ of datapoint $i$ is 0, and the model predicts a probability $p_i=1$ that this datapoint belongs to Class 1. Even though our model has made the worst possible prediction, the squared loss is only $(0-1)^2=1$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat_loss = pd.DataFrame({\"p_hat\": np.arange(0.001, 0.999, 0.01)}) \n",
    "p_hat_loss[\"L2 Loss\"] = (1 - p_hat_loss[\"p_hat\"])**2\n",
    "px.line(p_hat_loss, x=\"p_hat\", y=\"L2 Loss\", width=800,\n",
    "        title=\"Squared Loss for One Individual when y=1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating Cross-Entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a new loss, called the **negative log loss**, for when our **true observation is 1**. We define the loss on a single datapoint as $l = -\\log{p}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat_loss[\"Neg Log Loss\"] = -np.log(p_hat_loss[\"p_hat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(p_hat_loss.melt(id_vars=\"p_hat\", value_name=\"Loss\"), \n",
    "        x=\"p_hat\", y=\"Loss\", color=\"variable\", width=800,\n",
    "        title=\"Loss Comparison for One Observation when y = 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this penalizes wrong predictions far more than squared loss does.\n",
    "\n",
    "How to read this plot: Suppose the observation we're trying to predict is actually in **Class 1**. \n",
    "\n",
    "If our model gives an 80% chance of being in Class 1, the loss is relatively small (around 0.25). \n",
    "\n",
    "* If we predict only a 40% chance of being in Class 1, the loss is larger (around 1).\n",
    "* If we predict only a 5% chance of being in Class 1, the loss is 3.\n",
    "* And if we give a 0% chance of being in Class 1, the loss is **infinite**!\n",
    "\n",
    "What about when the true observation is 0? Consider the single-datapoint loss given by $l=-\\log{(1-p)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_hat_loss = pd.DataFrame({\"p_hat\": np.arange(0.001, 0.999, 0.01)}) \n",
    "p_hat_loss[\"L2 Loss\"] = (1 - (1-p_hat_loss[\"p_hat\"]))**2\n",
    "p_hat_loss[\"Neg Log Loss\"] = -np.log(1 - p_hat_loss[\"p_hat\"])\n",
    "px.line(p_hat_loss.melt(id_vars=\"p_hat\", value_name=\"Loss\"), \n",
    "        x=\"p_hat\", y=\"Loss\", color=\"variable\", width=800,\n",
    "        title=\"Loss Comparison for One Observation when y = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much of the formal derivation is in the slides. The equation for **cross-entropy loss** for a single observation is:\n",
    "\n",
    "$$\\textrm{loss}(y,\\hat{y}) = \\textcolor{red}{-y \\log(\\hat{y})} \\textcolor{orange}{- (1-y)\\log(1-\\hat{y})} = -\\left(\\textcolor{red}{y \\log(\\hat{y})} + \\textcolor{orange}{(1-y)\\log(1-\\hat{y})}\\right)$$\n",
    "\n",
    "Notice that the $\\textcolor{red}{\\text{red}}$ term is zeroed out (i.e., ignored) when $y=0$, and the $\\textcolor{orange}{\\text{orange}}$ term is zeroed out when $y=1$.\n",
    "\n",
    "- So, this one equation can be used to compute loss in scenarios where $y=1$ or $y=0$. We do not need two different equations.\n",
    "\n",
    "In logistic regression, we define $\\textcolor{cyan}{\\hat{y} = \\sigma(x^T \\theta)}$. So, the expression for **average** cross-entropy loss is:\n",
    "\n",
    "$$R(\\theta) = -\\frac{1}{n} \\sum_{i = 1}^n \\textcolor{red}{\\left[ y_i \\log (\\textcolor{cyan}{\\sigma(\\mathbb{X}_i^T \\theta)} \\right]} + \\textcolor{orange}{\\left[ (1 - y_i) \\log (1 - \\textcolor{cyan}{\\sigma(\\mathbb{X}_i^T \\theta)}) \\right]}$$\n",
    "\n",
    "Let's look at the loss surface for average cross-entropy loss on our toy data from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the cross entropy loss for a single data point and prediction\n",
    "def cross_entropy(y, p_hat):\n",
    "    return - y * np.log(p_hat) - (1 - y) * np.log(1 - p_hat)\n",
    "\n",
    "# Compute the average cross entropy loss for a particular theta\n",
    "def mean_cross_entropy_on_toy_data(theta):\n",
    "    p_hat = sigmoid(toy_df[\"x\"] * theta)\n",
    "    return np.mean(cross_entropy(toy_df[\"y\"], p_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_loss[\"Cross-Entropy\"] = theta_loss[\"theta\"].apply(mean_cross_entropy_on_toy_data).dropna()\n",
    "px.line(theta_loss, x=\"theta\", y=\"Cross-Entropy\", width=800,\n",
    "           title=\"Cross-Entropy on Toy Classification Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on where we run this code, we will get some error messages and strange behavior for extreme values of $\\theta$.  While the above equations are correct, they are not numerically stable.  We need to rewrite the loss function in a more numerically stable way.\n",
    "\n",
    "**The following derivation is out-of-scope for Data 100 but good to know for life.**\n",
    "\n",
    "The following is a more numerically stable implementation of the cross-entropy loss let $z = \\mathbb{X}_i^T \\theta$ and using the identity $\\log( 1-\\sigma(z)) = -z + \\log(\\sigma(z))$:\n",
    "\n",
    "\\begin{align}\n",
    "R(\\theta) \n",
    "&= -\\frac{1}{n} \\sum_{i = 1}^n \\left(y_i \\log (\\sigma(z)) + (1 - y_i) \\log (1 - \\sigma(z))\\right)\\\\\n",
    "&= -\\frac{1}{n} \\sum_{i = 1}^n \\left(y_i \\log (\\sigma(z)) + (1 - y_i) \\left( -z + \\log (\\sigma(z))\\right)\\right)\\\\\n",
    "&= -\\frac{1}{n} \\sum_{i = 1}^n \\left(y_i \\log (\\sigma(z)) - z + \\log \\left(\\sigma(z) \\right)+ y_i z  - y_i\\log \\left(\\sigma(z) \\right)  \\right)\\\\\n",
    "&= -\\frac{1}{n} \\sum_{i = 1}^n \\left( \\left(y_i - 1 \\right)z + \\log \\left(\\sigma(z) \\right)   \\right)\\\\\n",
    "\\end{align}\n",
    "\n",
    "We can further optimize this by using the identity $\\log(\\sigma(z)) = -\\log(1 + e^{-z})$ and applying more numerically stable log implementations:\n",
    "\n",
    "\\begin{align}\n",
    "R(\\theta) \n",
    "&= -\\frac{1}{n} \\sum_{i = 1}^n \\left( \\left( y_i - 1\\right)z - \\log(1 + e^{-z})   \\right)\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_cross_entropy_on_toy_data(theta):\n",
    "    y = toy_df[\"y\"]\n",
    "    z = toy_df[\"x\"] * theta\n",
    "    # using the log1p numerically stable operation\n",
    "    return -np.mean((y - 1) * z - np.log1p(np.exp(-z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_loss[\"Cross-Entropy\"] = theta_loss[\"theta\"].apply(mean_cross_entropy_on_toy_data).dropna()\n",
    "px.line(theta_loss.melt(id_vars=\"theta\", value_name=\"Loss\"), \n",
    "        x=\"theta\", y=\"Loss\", color=\"variable\",\n",
    "           title=\"Cross-Entropy on Toy Classification Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot is convex!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ce_theta = minimize(mean_cross_entropy_on_toy_data, x0 = -5)[\"x\"][0]\n",
    "best_ce_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(theta_loss.melt(id_vars=\"theta\", value_name=\"Loss\"), \n",
    "              x=\"theta\", y=\"Loss\", color=\"variable\",\n",
    "              title=\"Cross-Entropy on Toy Classification Data\")\n",
    "fig.add_scatter(x=[best_theta], y=[mse_on_toy_data(best_theta)], \n",
    "                mode=\"markers\", marker_size=10, marker_color=\"red\",\n",
    "                name=f\"Theta_1: {best_theta:.2f}\")\n",
    "fig.add_trace(go.Scatter(x=[best_ce_theta], y=[mean_cross_entropy_on_toy_data(best_ce_theta)], \n",
    "                         mode=\"markers\", marker_size=10, marker_color=\"Blue\",\n",
    "                         name=f\"CE Theta: {best_ce_theta:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can see what our new model looks like using the correct loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(toy_df, x=\"x\", y=\"y\", color=\"str_y\", width=800)\n",
    "xs = np.linspace(-10, 10, 100)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=sigmoid(xs * best_theta), \n",
    "    mode=\"lines\", line_color=\"red\", \n",
    "    name=f\"LR + MSE Loss\"))\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=xs, y=sigmoid(xs * best_ce_theta), \n",
    "    mode=\"lines\", line_color=\"blue\", \n",
    "    name=f\"LR + CE Loss\"))\n",
    "\n",
    "fig.update_traces(marker_size=20)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
