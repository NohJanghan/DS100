{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí¨ Lecture 6, Regex ‚Äì Data 100, Summer 2025\n",
    "\n",
    "Data 100, Summer 2025\n",
    "\n",
    "[Acknowledgments Page](https://ds100.org/su25/acks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### ü§† Text Wrangling and Regex\n",
    "\n",
    "Working with text: applying string methods and regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import zipfile\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü•´ Demo 1: Canonicalizing County Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pd.read_csv(\"data/county_and_state.csv\")\n",
    "populations = pd.read_csv(\"data/county_and_population.csv\")\n",
    "\n",
    "# display() allows us to view a DataFrame without returning it as an object\n",
    "display(states)\n",
    "display(populations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these DataFrames share a \"County\" column. Unfortunately, formatting differences mean that we can't directly merge the two DataFrames using the \"County\"s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.merge(populations, left_on=\"County\", right_on=\"County\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üêº Using Pandas String Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, we can **canonicalize** the \"County\" string data to apply a common formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to transform a series of county names into a standard form\n",
    "def canonicalize_county(county_series):\n",
    "    canonicalized_series = (\n",
    "        county_series\n",
    "        # make lowercase\n",
    "        .str.lower()   \n",
    "        # remove spaces            \n",
    "        .str.replace(' ', '')    \n",
    "        # replace & with and               \n",
    "        .str.replace('&', 'and')   \n",
    "         # remove dots             \n",
    "        .str.replace('.', '')       \n",
    "        # remove \"county\"           \n",
    "        .str.replace('county', '')\n",
    "        # remove \"parish\" \n",
    "        .str.replace('parish', '')              \n",
    "    )\n",
    "    return (canonicalized_series)\n",
    "\n",
    "display(canonicalize_county(states[\"County\"]))\n",
    "display(canonicalize_county(populations[\"County\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[\"Canonical County\"] = canonicalize_county(states[\"County\"])\n",
    "\n",
    "populations[\"Canonical County\"] = canonicalize_county(populations[\"County\"])\n",
    "\n",
    "display(states)\n",
    "display(populations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the merge works as expected!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.merge(populations, on=\"Canonical County\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "**Instructor note: Return to Lecture!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ü™µ Demo 2: Extracting Data from Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample log file\n",
    "log_fname = 'data/log.txt'\n",
    "\n",
    "with open(log_fname, 'r') as f:\n",
    "    # readlines() returns a list of strings, \n",
    "    # with each element representing a line in the file\n",
    "    log_lines = f.readlines()\n",
    "    \n",
    "log_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to extract the day, month, year, hour, minutes, seconds, and timezone. \n",
    "\n",
    "- Looking at the data, we see that these items are not in a fixed position relative to the beginning of the string. \n",
    "\n",
    "- In other words, slicing by some fixed offset isn't going to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20:31 were determined by trial-and-error!\n",
    "log_lines[0][20:31] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we use the same range for the next log line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_lines[1][20:31]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, we'll need to use some more sophisticated thinking. Let's focus on only the first line of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first = log_lines[0]\n",
    "first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the data inside the square brackes by splitting the string at `[` and `]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the text enclosed in square brackets\n",
    "pertinent = (\n",
    "\n",
    "    # remove everything before the first [\n",
    "    first.split(\"[\")[1] \n",
    "\n",
    "    # Remove everything after the second square ]\n",
    "    .split(']')[0] \n",
    "\n",
    ") \n",
    "\n",
    "pertinent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the date, month, and the rest of the pertinent string (`rest`)\n",
    "day, month, rest  = pertinent.split('/')        \n",
    "\n",
    "print(\"Day:   \", day)\n",
    "print(\"Month: \", month)\n",
    "print(\"Rest:  \", rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from `rest`, grab the year, hour, minute, and remaining characters\n",
    "year, hour, minute, rest2 = rest.split(':')    \n",
    "\n",
    "print(\"Year:   \", year)\n",
    "print(\"Hour:   \", hour)\n",
    "print(\"Minute: \", minute)\n",
    "print(\"Rest:   \", rest2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from `rest2`, grab the seconds and time zone\n",
    "seconds, time_zone = rest2.split(' ') \n",
    "print(\"Seconds:   \", seconds)\n",
    "print(\"Time Zone:   \", time_zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the components we've extracted\n",
    "day, month, year, hour, minute, seconds, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating the process above, but simultaenously for all lines of the log file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pd.read_csv(\"data/log.txt\", \n",
    "                sep=\"\\t\", \n",
    "                header=None)[0]\n",
    "\n",
    "print(\"Original input:\")\n",
    "display(logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous code:\n",
    "# first = '26/Jan/2014:10:47:58 -0800'\n",
    "# pertinent = first.split(\"[\")[1].split(']')[0]\n",
    "\n",
    "s1 = (\n",
    "  logs.str.split(\"[\")\n",
    "      .str[1]\n",
    "      .str.split(\"]\")\n",
    "      .str[0]\n",
    ")\n",
    "display(s1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous code:\n",
    "# day, month, rest  = pertinent.split('/') \n",
    "\n",
    "df1 = (\n",
    "  # expand=True creates a column for each element of the split\n",
    "  s1.str.split(\"/\", expand=True)\n",
    "  .rename(columns={0: \"Day\", 1: \"Month\", 2: \"Rest\"})\n",
    ")\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous code:\n",
    "# year, hour, minute, rest2 = rest.split(':') \n",
    "\n",
    "rest_df = (\n",
    "  df1[\"Rest\"].str.split(\":\", expand=True)\n",
    "  .rename(columns={0: \"Year\", 1: \"Hour\", 2: \"Minute\", 3: \"Rest2\"})\n",
    ")\n",
    "display(rest_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (\n",
    "  # merge based on the index, not a particular column\n",
    "  df1.merge(rest_df, left_index=True, right_index=True)\n",
    "  .drop(columns=[\"Rest\"])\n",
    ")\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous code:\n",
    "# seconds, time_zone = rest.split(' ')\n",
    "\n",
    "rest2_df = (\n",
    "  df2[\"Rest2\"].str.split(\" \", expand=True)\n",
    "  .rename(columns = {0: \"Seconds\", 1: \"Timezone\"})\n",
    ")\n",
    "rest2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = (\n",
    "    df2.merge(rest2_df, left_index=True, right_index=True)\n",
    "    .drop(columns=[\"Rest2\"])\n",
    ")\n",
    "\n",
    "print(\"Final Dataframe:\")\n",
    "display(df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may see code like this in data cleaning pipelines.  \n",
    "\n",
    "However, **regular expressions** provide a faster and more expressive mechanism to extract strings that match certain patterns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> <br>\n",
    "\n",
    "**Instructor note: Return to lecture!**\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üí¨ Regular Expressions\n",
    "\n",
    "**[regex101.com](http://regex101.com/) is a great place to experiment with regular expressions!**\n",
    "\n",
    "Quadruple blackslash example from slides:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints newline\n",
    "print('Printing one backslash (Note the extra linespace!):')\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints \\n\n",
    "print('Printing two backslashes:')\n",
    "print('\\\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints \\ followed by newline\n",
    "print('Printing three backslashes (Note the extra linespace!):')\n",
    "print('\\\\\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints \\\\n\n",
    "print('Printing four backslashes:')\n",
    "print('\\\\\\\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also prints \\\\n, but much more obviously!\n",
    "print('Raw string with two backslashes:')\n",
    "print(r'\\\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lesson: Use raw strings to simplify regular expressions in Python! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## üéª String Extraction with Regex\n",
    "\n",
    "Python `re.findall` returns a list of all extracted matches from a **single string**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"My social security number is 123-45-6789 bro, or actually maybe it‚Äôs 321-45-6789.\";\n",
    "\n",
    "pattern = r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\n",
    "\n",
    "re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Now, let's see vectorized extraction in `pandas`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " `.str.findall` returns a `Series` of lists of all matches in each record.\n",
    "\n",
    " - In other words, it effectively applies `re.findall` to each element of the `Series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ssn = pd.DataFrame(\n",
    "    ['987-65-4321',\n",
    "     'forty',\n",
    "     '123-45-6789 bro or 321-45-6789',\n",
    "     '999-99-9999'],\n",
    "    columns=['SSN'])\n",
    "df_ssn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series of lists\n",
    "pattern = r\"[0-9]{3}-[0-9]{2}-[0-9]{4}\"\n",
    "df_ssn['SSN'].str.findall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting individual matches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For example, grab the final match from each list\n",
    "(\n",
    "  df_ssn['SSN']\n",
    "  .str.findall(pattern)\n",
    "  .str[-1] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "**Instructor note: Return to slides!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<br> <br>\n",
    "\n",
    "\n",
    "## üß≤ Extraction Using Regex Capture Groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python function `re.findall`, in combination with parentheses returns specific substrings (i.e., **capture groups**) within each matched string, or **match**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I will meet you at 08:30:00 pm tomorrow\"\"\"       \n",
    "pattern = \".*(\\d\\d):(\\d\\d):(\\d\\d).*\"\n",
    "matches = re.findall(pattern, text)\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The three capture groups in the first matched string\n",
    "hour, minute, second = matches[0]\n",
    "print(\"Hour:   \", hour)\n",
    "print(\"Minute: \", minute)\n",
    "print(\"Second: \", second)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "In `pandas`, we can use `.str.extract` to extract each capture group of **only the first match** of each record into separate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back to SSNs\n",
    "df_ssn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will extract the first match of all groups\n",
    "pattern_group_mult = r\"([0-9]{3})-([0-9]{2})-([0-9]{4})\" # 3 groups\n",
    "df_ssn['SSN'].str.extract(pattern_group_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, `.str.extractall` extracts **all matches** of each record into separate columns. Rows are then **MultiIndexed** by original record index and match index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame, one row per match\n",
    "df_ssn['SSN'].str.extractall(pattern_group_mult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "**Instructor note: Return to Slides!**\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ü•´ Canonicalization with Regex (`re.sub`, `Series.str.replace`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In regular Python, canonicalize with `re.sub` (\"substitute\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '<div><td valign=\"top\">Moo</td></div>'\n",
    "pattern = r\"<[^>]+>\"\n",
    "re.sub(pattern, '', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "In `pandas`, canonicalize with `Series.str.replace`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example dataframe of strings\n",
    "df_html = pd.DataFrame(\n",
    "  [\n",
    "    '<div><td valign=\"top\">Moo</td></div>',\n",
    "    '<a href=\"http://ds100.org\">Link</a>',\n",
    "    '<b>Bold text</b>'\n",
    "  ], \n",
    "  columns=['Html'])\n",
    "\n",
    "df_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series -> Series\n",
    "df_html[\"Html\"].str.replace(pattern, '', regex=True).to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "**Instructor note: Return to lecture!**\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéÅ (Optional) Bonus material\n",
    "\n",
    "None of the code below is covered during lecture. Nonetheless, you may find \n",
    "it helpful to review this code, as it's a nice example application of the regex\n",
    "functions from lecture, and it's relevant to the homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "\n",
    "# ü™µ Revisiting Text Log Processing using Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python `re` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = log_lines[0]\n",
    "display(line)\n",
    "\n",
    "pattern = r'\\[(\\d+)\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "day, month, year, hour, minute, second, time_zone = re.findall(pattern, line)[0] # get first match\n",
    "day, month, year, hour, minute, second, time_zone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `pandas` version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(log_lines, columns=['Log'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Option 1: `Series.str.findall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\\[(\\d+)\\/(\\w+)\\/(\\d+):(\\d+):(\\d+):(\\d+) (.+)\\]'\n",
    "df['Log'].str.findall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "\n",
    "Option 2: `Series.str.extractall`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Log'].str.extractall(pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling either of these two DataFrames into a nice format (like below) is left as an exercise for you! You will do a related problem on the homework.\n",
    "\n",
    "\n",
    "||Day|Month|Year|Hour|Minute|Second|Time Zone|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|0|26|Jan|2014|10|47|58|-0800|\n",
    "|1|2|Feb|2005|17|23|6|-0800|\n",
    "|2|3|Feb|2006|10|18|37|-0800|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here (optional)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "<br/>\n",
    "\n",
    "---\n",
    "\n",
    "# üçΩÔ∏è Real World Case Study: Restaurant Data\n",
    "\n",
    "In this example, we will show how regexes can allow us to track quantitative data across categories defined by the appearance of various text fields.\n",
    "\n",
    "In this example we'll see how the presence of certain keywords can affect quantitative data:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio = pd.read_csv('data/violations.csv', header=0, names=['bid', 'date', 'desc'])\n",
    "desc = vio['desc']\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = desc.value_counts()\n",
    "counts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a lot of different descriptions!! Can we **canonicalize** at all? Let's explore two sets of 10 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hmmm...\n",
    "counts[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use regular expressions to cut out the extra info in square braces.\n",
    "vio['clean_desc'] = (vio['desc']\n",
    "             .str.replace(r'\\s*\\[.*\\]$', '', regex=True)\n",
    "             .str.strip()       # removes leading/trailing whitespace\n",
    "             .str.lower())\n",
    "vio.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonicalizing definitely helped\n",
    "vio['clean_desc'].value_counts().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vio['clean_desc'].value_counts().head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our research question:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Below, we use regular expressions and `df.assign()` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.assign.html?highlight=assign#pandas.DataFrame.assign)) to **method chain** our creation of new boolean features, one per keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expressions to assign new features for the presence of various keywords\n",
    "# regex metacharacter | \n",
    "with_features = (vio\n",
    " .assign(is_unclean     = vio['clean_desc'].str.contains('clean|sanit'))\n",
    " .assign(is_high_risk = vio['clean_desc'].str.contains('high risk'))\n",
    " .assign(is_vermin    = vio['clean_desc'].str.contains('vermin'))\n",
    " .assign(is_surface   = vio['clean_desc'].str.contains('wall|ceiling|floor|surface'))\n",
    " .assign(is_human     = vio['clean_desc'].str.contains('hand|glove|hair|nail'))\n",
    " .assign(is_permit    = vio['clean_desc'].str.contains('permit|certif'))\n",
    ")\n",
    "with_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "### üìä EDA\n",
    "\n",
    "That's the end of our text wrangling. Now let's do some more analysis to analyze restaurant health as a function of the number of violation keywords.\n",
    "\n",
    "To do so we'll first group so that our **granularity** is one inspection for a business on particular date. This effectively counts the number of violations by keyword for a given inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features = (with_features\n",
    " .groupby(['bid', 'date'])\n",
    " .sum(numeric_only=True)\n",
    " .reset_index()\n",
    ")\n",
    "count_features.iloc[255:260, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out our new dataframe in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_features[count_features['is_vermin'] > 1].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll reshape this \"wide\" table into a \"tidy\" table using a pandas feature called `pd.melt` ([documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.melt.html?highlight=pd%20melt)) which we won't describe in any detail, other than that it's effectively the inverse of `pd.pivot_table`.\n",
    "\n",
    "Our **granularity** is now a violation type for a given inspection (for a business on a particular date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_type_df = pd.melt(count_features, id_vars=['bid', 'date'],\n",
    "            var_name='feature', value_name='num_vios')\n",
    "\n",
    "# show a particular inspection's results\n",
    "violation_type_df[(violation_type_df['bid'] == 489) & (violation_type_df['date'] == 20150728)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember our research question:\n",
    "\n",
    "> **How do restaurant health scores vary as a function of the number of violations that mention a particular keyword?** \n",
    "> <br/>\n",
    "> (e.g., unclean surfaces, vermin, permits, etc.)\n",
    "\n",
    "<br/>\n",
    "\n",
    "We have the second half of this question! Now let's **join** our table with the inspection scores, located in `inspections.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the scores\n",
    "inspection_df = pd.read_csv('data/inspections.csv',\n",
    "                  header=0,\n",
    "                  usecols=[0, 1, 2],\n",
    "                  names=['bid', 'score', 'date'])\n",
    "inspection_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the inspection scores were stored in a separate file from the violation descriptions, we notice that the **primary key** in inspections is (`bid`, `date`)! So we can reference this key in our join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join scores with the table broken down by violation type\n",
    "violation_type_and_scores = (\n",
    "    violation_type_df\n",
    "    .merge(inspection_df, on=['bid', 'date'])\n",
    ")\n",
    "violation_type_and_scores.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/><br/>\n",
    "\n",
    "---\n",
    "\n",
    "Let's plot the distribution of scores, broken down by violation counts, for each inspection feature (`is_clean`, `is_high_risk`, `is_vermin`, `is_surface`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you will learn this syntax next week. Focus on interpreting for now.\n",
    "sns.catplot(x='num_vios', y='score',\n",
    "               col='feature', col_wrap=2,\n",
    "               kind='box',\n",
    "               data=violation_type_and_scores);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can observe:\n",
    "* The inspection score generally goes down with increasing numbers of violations, as expected.\n",
    "* Depending on the violation keyword, inspections scores on average go down at slightly different rates.\n",
    "* For example, that if a restaurant inspection involved 2 violations with the keyword \"vermin\", the average score for that inspection would be a little bit below 80."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
